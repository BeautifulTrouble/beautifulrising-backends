#!/usr/bin/env python3
#encoding: utf-8

# Auto-install and activate a virtualenv if possible
import autovenv
autovenv.run()

import argparse
import os
import re
import sys
from datetime import datetime
from itertools import zip_longest
from pprint import pprint
from string import capwords
from urllib.parse import urljoin

import archieml
import couchdb
import driveclient
import jinja2
import langdetect
import requests
from fuzzywuzzy.process import extractOne as fuzzy

from utils import *


DB_NAME = 'toolbox'
DB_SERVER = 'http://127.0.0.1:5984/'

CONFIG_FILE_NAME = 'CONFIG'
ROOT_FOLDER_NAME = 'BR CONTENT'

DRIVE_CLIENT_NAME = 'beautiful-rising'
SERVICE_ACCOUNT_JSON_FILENAME = 'beautiful-rising-eae5ff71ae04.json'


class ContentLoader(object):
    def __init__(self):
        # Parse command line arguments
        arg_parser = argparse.ArgumentParser(description="")
        arg_parser_group = arg_parser.add_mutually_exclusive_group()
        arg_parser_group.add_argument('--id', type=str, metavar='DOCUMENT_ID', action='append', 
            default=[], dest='ids', help="Fetch a single document by its globally unique id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--change-id', type=str, metavar='CHANGE_ID', action='append', 
            default=[], dest='changes', help="Fetch a single document by its ephemeral change id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--assets', action='store_true', help="Download assets only")
        arg_parser_group.add_argument('--watch-docs', action='store_true',
            help="Initiate a request to watch drive for changes. It will expire in one day.")
        arg_parser_group.add_argument('--stop-watching', action='store_true', 
            help="If the db has any record of a watch request, request that it be cancelled.")
        arg_parser_group.add_argument('--report-broken-docs', action='store_true', 
            help="Produce a document containing information about documents which lack required "
            "fields for their type, as specified in the config document.")
        arg_parser_group.add_argument('--delete-db', action='store_true',
            help='Delete any existing database named "{}".'.format(DB_NAME))
        self.options,_ = arg_parser.parse_known_args()

        # Connect to couchdb
        self.couch = couchdb.Server(DB_SERVER)
        self.db = self.couch[DB_NAME] if DB_NAME in self.couch else self.couch.create(DB_NAME)

        # Connect to Google Drive and get the root folder
        self.drive = driveclient.DriveClient(DRIVE_CLIENT_NAME,
            scopes='https://www.googleapis.com/auth/drive',
            service_account_json_filename=SERVICE_ACCOUNT_JSON_FILENAME)
        self.root = self.drive.folder(ROOT_FOLDER_NAME) 
        if not self.root:
            die("Can't find the root folder!")

        # Load CONFIG_FILE_NAME from ROOT_FOLDER_NAME and store as "config:api" & self.config
        self.configure()

        # Download assets
        if self.options.assets:
            self.download_assets()

        # Watch for changes
        elif self.options.watch_docs:
            self.unwatch()
            self.watch()

        # Stop watching for changes
        elif self.options.stop_watching:
            self.unwatch()

        # Delete the database
        elif self.options.delete_db:
            confirm = input('Delete the database "{}" [y/N]? '.format(DB_NAME))
            if confirm.lower() == 'y':
                self.unwatch()
                del self.couch[DB_NAME]

        # Produce a report in the root folder about documents which lack required fields
        elif self.options.report_broken_docs:
            documents = self.get_documents()
            all_content = [c for c in map(self.extract_and_transform, documents) if c]
            all_content = self.merge_translations(all_content)
            all_content = self.filter_broken_content(all_content, report=True)

        # Load content
        else:
            #TODO: in production always unwatch/watch
            #TODO: handle document renaming/deletion

            # Fetch all the existing content from the db
            existing_content = {d.doc['slug']: d.doc for d in self.db.view('_all_docs', include_docs=True) 
                                if 'document_id' in d.doc}

            # Identify published documents by their filenames and fetch new content
            published = re.compile(self.config['published-filename-regex']).search
            published_documents = (d for d in self.get_documents() if published(d.title))
            new_content = filter(None, map(self.extract_and_transform, published_documents))

            # TODO: detect duplicate names

            # Merge new content with existing, preserving revision number and translations
            for content in new_content:
                existing = existing_content.get(content['slug'])
                #TODO: this is a bug and should be fixed (duplicate titles -> duplicate slugs -> duplicate ids -> incorrect reasoning about what is or isn't an existing piece)
                if existing and '_rev' in existing: 
                    content['_rev'] = existing['_rev']
                    content['translations'] = existing.get('translations', {})
                existing_content[content['slug']] = content

            all_content = list(existing_content.values())

            all_content = self.custom_filters(all_content)

            all_content = self.merge_translations(all_content)
            all_content = self.fix_relationships(all_content)
            all_content = self.filter_broken_content(all_content)
            self.download_assets()
            self.db_save(all_content)


    def custom_filters(self, all_content):
        '''
        Project-specific filtering 
        '''
        matcher = re.compile(r'(?<!!)\[([^\]]*)\]\(((?!http)[^)]+)\)').search
        xrefpat = self.config['xref-link-pattern']

        def patch_links(text):
            m, chunks = matcher(text), []
            while m:
                link_text, module_name, _, end = *m.groups(), *m.span()
                content = self.find_content(module_name, all_content, thresh=90)
                if content:
                    if link_text:
                        replacement = '[%s]({})'.format(xrefpat) % (link_text, content['slug'])
                    else:
                        replacement = '(see: [%s: %s]({}))'.format(xrefpat) % (content['type'].upper(), content['title'], content['slug'])
                    chunks.append(re.sub(re.escape(m.group()), replacement, text[:end]))
                else:
                    #TODO: do something clever for missing modules?
                    chunks.append(text[:end])
                text = text[end:]
                m = matcher(text)
            return ''.join(chunks) + text

        # Recursive visitor reaches all deeply nested strings
        visit_all = lambda x: {
            list:   lambda L: [visit_all(i) for i in L],
            dict:   lambda d: {k: visit_all(v) for k,v in d.items()},
            str:    lambda s: patch_links(s),
            int:    lambda i: i,
        }[dict if isinstance(x, dict) else type(x)](x)

        for content in all_content:
            # Clean up learn-more section
            learn = content.get('learn-more')
            if learn:
                content['learn-more'] = [L for L in learn 
                    if L.get('title') and L.get('link') and L.get('title') != 'abc' and L.get('link') != 'url']
            # Clean up real-world-examples section
            examples = content.get('real-world-examples')
            if examples:
                content['real-world-examples'] = [e for e in examples if all(map(e.get, ['title','link','description']))]
            # Patch up links between modules
            for field in self.config['markdown']:
                if content.get(field):
                    content[field] = visit_all(content.get(field))
        return all_content


    def parse_markup(self, text):
        '''
        Abstract all markup preprocessing and parsing to this function
        '''
        # What is this, macrosoft wandows?
        text = text.replace('\r', '')
        # Obliterate ALL of google's [a][b][c] comment annotations!
        text = re.sub(r'^\[[a-z]\].+$', '', text, flags=re.M)
        text = re.sub(r'\[[a-z]\]', '', text)
        # Undo some of the auto-capitalization google docs inflicts
        return {k.lower(): v for k,v in archieml.loads(text).items() if v}


    def type_info(self, content):
        '''
        Return the type data structure for the content item
        '''
        return next((T for T in self.config['types'] if T['name'] in content), {})


    def find_content(self, item_or_name, item_list, thresh=50, fuzzy_match_cache={}):
        '''
        Use fuzzy matching to find a content item from a list
        '''
        item_name = item_or_name
        if not isinstance(item_or_name, str):
            item_name = item_or_name['title']
        cached = fuzzy_match_cache.get(item_name)
        if cached:
            return cached
        match = fuzzy(item_name, item_list, processor=lambda m: m['title'])
        if match and match[1] >= thresh:
            fuzzy_match_cache[item_name] = match[0]
            return match[0]
        return {}


    def db_save(self, doc_or_docs):
        '''
        Write one or many dicts (docs) to couchdb
        '''
        if not doc_or_docs: return
        # Handle one or many docs
        docs = [doc_or_docs] if isinstance(doc_or_docs, dict) else doc_or_docs
        # Remove couch-disallowed keys and add _id where needed
        docs = [{k:v for k,v in d.items() if k in ('_id', '_rev') or not k.startswith('_')} for d in docs]
        [d.update(_id='{type}:{slug}'.format(**d)) for d in docs if '_id' not in d]
        # Simple conflict resolution (WARNING: this won't work with replication!)
        for success,id,rev_or_exc in self.db.update(docs):
            if isinstance(rev_or_exc, couchdb.http.ResourceConflict):
                retry = [d for d in docs if d['_id'] == id][0]
                retry.update(_rev=self.db[id]['_rev'])
                self.db.save(retry)


    def configure(self):
        '''
        Fetch, parse, set defaults, and store the config
        '''
        # Cap couchdb revision limit since documents are so frequently updated
        requests.put(urljoin(DB_SERVER, DB_NAME+'/_revs_limit'), data='50').status_code

        # Load configuration document and set defaults
        document = self.root.file(CONFIG_FILE_NAME)
        if not document:
            die("Can't find a config file!")
        self.config = c = self.parse_markup(document.text)

        # Hiding configuration options from the API
        c.setdefault('config-hidden-keys', ['notification-token'])
        # Drive notifications
        c.setdefault('notification-url', '')
        c.setdefault('notification-token', '')
        # Language settings
        c.setdefault('language-default', 'en')
        c.setdefault('language-all', ['en'])
        c.setdefault('language-add', '')
        c.setdefault('language-remove', '')
        # How we distinguish published content
        c.setdefault('published-filename-regex', r'(?i)\bdone$')
        # Renaming synonymous keys, including those with language-suffixes
        c.setdefault('synonyms', {})
        # Shuffling content together
        c.setdefault('plural-separator-regex', r'(?:\s*,|\s+and|\s+&)\s+')
        c.setdefault('plural-keys', {})
        c.setdefault('plural-keys-collate', {})
        # Relationships between content items
        c.setdefault('relationships', {})
        # Fields which should be parsed with markdown and "normalization"
        c.setdefault('normalize', [])
        c.setdefault('markdown', [])
        c.setdefault('xref-link-pattern', '/module/%s')
        # Fields which should be indexed by client search engines
        c.setdefault('search', [])
        # Download top-level assets from this top-level folder to this (relative) local path
        c.setdefault('asset-sources', ['ASSETS'])
        c.setdefault('asset-path', '/assets/content')
        # Content type information
        c['types'] = []
        for key,value in c.items():
            if key.startswith('types-'):
                for type in value:
                    type['name'] = slugify(type['one'])
                c['types'] += value

        # Save the config before creating lots of temporary language-related data within it
        c.update(type='config', slug='api')
        self.db_save(c)
        log('load: configuration options from drive document "{}"'.format(CONFIG_FILE_NAME))

        # Key transformations have to take into account language suffixes, so this adds suffixed copies
        # of synonyms, plural-keys, and plural-keys-collate (where each entry is a nested list).
        add_language_suffixes = lambda D: [D.update(each) for each in
            [{k+'-'+lang: [i+'-'+lang for i in v] if isinstance(v, list) else
                          v+'-'+lang for k,v in D.items()} for lang in self.config['language-all']] ]
        add_language_suffixes(c['synonyms'])
        add_language_suffixes(c['plural-keys'])
        add_language_suffixes(c['plural-keys-collate'])


    def watch(self):
        '''
        Request push notifications for entire drive be sent to the notification-url
        '''
        now = datetime.utcnow()
        expiration = int((60*60*24 + now.timestamp()) * 1000) # UTC + 24h in ms
        self.drive.execute(self.drive.service.changes().watch(body={
            'id': '{}-{}'.format(DRIVE_CLIENT_NAME, expiration),
            'type': 'web_hook',
            'address': self.config['notification-url'],
            'token': self.config['notification-token'],
            'expiration': expiration,
        }))
        log("watch: for push notifications at {}".format(self.config['notification-url']))
    
    
    def unwatch(self):
        '''
        Request all push notification channels in db be cancelled
        '''
        if 'config:notification-channels' in self.db:
            for channel,resource in self.db['config:notification-channels'].items():
                if channel not in ('_id', '_rev'):
                    try:
                        self.drive.execute(self.drive.service.channels().stop(body={
                            'id': channel, 
                            'resourceId': resource,
                        }))
                        warn("stop: Channel-Id: {} Resource-Id: {}".format(channel, resource))
                    except: pass
            del self.db['config:notification-channels']


    def get_documents(self):
        '''
        Get documents by file ids, change ids or all documents
        '''
        documents = []
        # Get only the specifically requested documents by id or change id
        if self.options.ids or self.options.changes:
            documents.extend(d for d in (self.drive.get(id) for id in self.options.ids) if d)
            documents.extend(d for d in (self.drive.get_change(id) for id in self.options.changes) if d)
        # Get all documents
        else:
            # Recursive folder getter requires python3.3+ for "yield from"
            def get_folders(root):
                for folder in root.folders:
                    yield folder
                    yield from get_folders(folder)
            for folder in get_folders(self.root):
                documents.extend(folder.documents)
                log('find: content in drive folder "{}"'.format(folder.title))
        return documents


    def extract_and_transform(self, document):
        '''
        Process a document and return a content item.
        '''
        content = self.parse_markup(document.text)

        # Rename synonymous keys (this should happen before all other transformations)
        for old_key,new_key in self.config['synonyms'].items():
            old_value = content.get(old_key)
            if old_value is not None:
                content[new_key] = old_value
                del content[old_key]

        # Determine the type
        type = self.type_info(content).get('name', '')
        content['type'] = type
        content['title'] = title = content.get(type)
        if not isinstance(title, str): 
            warn("skip: {} bad type information".format(document.id))
            return

        # Add a few useful bits
        content['slug'] = slugify(content.get('title', ''), allow=':')
        content['timestamp'] = int(1000 * datetime.strptime(document.modifiedDate, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp())
        content['document_id'] = document.id
        content['document_link'] = document.alternateLink
        content['document_title'] = document.title

        # Convert singular keys to plural keys and split them up as lists
        for plural_key,singular_key in self.config['plural-keys'].items():
            single, plural = content.get(singular_key), content.get(plural_key)
            if single:
                content[plural_key] = [single]
                if plural_key != singular_key:
                    del content[singular_key]
            if plural and not isinstance(plural, list):
                multiline = re.split(r'\s*\n\s*\n\s*', plural)
                content[plural_key] = (multiline if len(multiline) > 1 else 
                                       re.split(self.config['plural-separator-regex'], plural))

        # Collate (zip) specific plural data together and give it new names
        for output,plurals in self.config['plural-keys-collate'].items():
            collated_lists = zip_longest(*(content.get(existing_key, []) for existing_key in plurals), fillvalue='')
            # If the output key has a language suffix, remove it from the inner keys
            if re.search('-(?:{})$'.format('|'.join(self.config['language-all'])), output):
                sans_suffix = [self.config['plural-keys'][p][:-3] for p in plurals] 
                collated_dicts = [dict(zip(sans_suffix, L)) for L in collated_lists]
            # Otherwise create the inner keys normally
            else:
                collated_dicts = [dict(zip(map(self.config['plural-keys'].get, plurals), L)) for L in collated_lists]
            for old_key in plurals:
                content.pop(old_key, None)
            if collated_dicts:
                content[output] = collated_dicts

        # Apply the "normalization" filter which is curently just capitalized words
        for field in self.config['normalize']:
            obj = content.get(field)
            if obj:
                content[field] = [capwords(w) for w in obj] if isinstance(obj, list) else capwords(obj)
        
        log("extract: {} ({}: {})".format(document.id, type, content['title']))
        return content


    def add_language_tags(self, all_content):
        '''
        Detect the language of each content item and add a language tag
        '''
        language_suffixes = ['-'+lang for lang in self.config['language-all']]
        for content in all_content:
            if 'lang' not in content:
                corpus = '\n'.join(v for k,v in content.items() if isinstance(v, str)
                                   and (k[-3:] not in language_suffixes)
                                   and (not k.startswith('document-')))
                try:
                    lang = langdetect.detect(corpus)
                    assert lang in self.config['language-all']
                except:
                    lang = self.config['language-default']
                content['lang'] = lang


    def merge_translations(self, all_content):
        '''
        Assuming all necessary documents have already been fetched, merge translations
        into the content object for the default language. They will be placed into a 
        'translations' dictionary under two-letter language code keys.
        '''
        content_primary = []
        content_translated = []

        # Sort content by language
        self.add_language_tags(all_content)
        for content in all_content:
            if content['lang'] == self.config['language-default']:
                content_primary.append(content)
            else:
                content_translated.append(content)

        # Add translated content to a translations dict in each default language piece
        for translation in content_translated:
            default = self.find_content(translation.get('default-language-content', ''), content_primary, thresh=90)
            if not default:
                warn("skip: {} can't find default language version {}".format(translation.get('title'), translation.get('default-language-content')))
                continue
            translations = default.setdefault('translations', {})
            translations[translation['lang']] = translation
            log("merge: {} ({}) => {}".format(translation['title'], translation['lang'], default['title']))

        # Look through primary language documents and move translated keys into place
        for content in content_primary:
            content.setdefault('translations', {})
            for lang in self.config['language-all']:
                keys = [k for k in content if isinstance(k, str) and k.endswith('-'+lang)]
                inline = {k[:-3]: content[k] for k in keys if content[k]}
                if inline:
                    if lang == self.config['language-default']:
                        content.update(inline)
                    else:
                        lang_dict = content['translations'].setdefault(lang, {})
                        lang_dict.update(inline)
                    [content.pop(k) for k in keys]
                    print(lang, '-=>', inline) #TODO: remove

        # All content is now in this merged list
        return content_primary


    def fix_relationships(self, all_content):
        '''
        Replace relationships based on titles with fuzzy-matched slugs
        '''
        # Create a mapping between keys and possible content items
        typed = {}
        for content in all_content:
            typed.setdefault(content['type'], []).append(content)

        relationships = self.config['relationships']
        keys_and_choices = {k: typed.get(v, []) for k,v in relationships.items()}
        types_and_keys = {v: k for k,v in relationships.items()}

        # Fuzzy match and slugify related content items
        for content in all_content:
            this_slug = {content['slug']}
            key_referring_to_this_type = types_and_keys.get(content['type'])
            for key,choices in keys_and_choices.items():
                if key in content:
                    found_content = {c['slug']: c for c in (self.find_content(t, choices, 90)
                                                            for t in content[key]) if c}
                    content[key] = sorted(found_content)
                    # If anything else relates to this object's type, make relationships bi-directional
                    if key_referring_to_this_type:
                        for other in found_content.values():
                            other_list = other.get(key_referring_to_this_type, [])
                            other[key_referring_to_this_type] = sorted(this_slug | set(other_list))
        return all_content


    def filter_broken_content(self, all_content, report=False):
        '''
        Remove content with obvious content problems from all_content
        '''
        for content in all_content:
            required_sets = [r.split('|') for r in self.type_info(content).get('required', [])]
            missing_items = [' or '.join(s) for s in required_sets if not any(content.get(k) for k in s)]
            if missing_items:
                content['missing-items'] = missing_items
                warn('missing:', content['title'], missing_items)
        good_content = [c for c in all_content if 'missing-items' not in c]

        if report:
            title = 'Content Error Report'
            timestamp = datetime.utcnow()
            html = jinja2.Template('''
                <style>* { font-family: "Consolas"; }</style>
                <h1>{{ title }}</h1>
                <p>
                    Modifications to this file will be discarded.<br>
                    Translated pieces are not currently represented in this report.<br>
                    Be aware that a field flagged as missing may indicate that the<br>
                    related document to which it refers is missing or not "DONE."<br>
                    <em>Updated: {{ timestamp }} UTC</em><br>
                </p>
                <hr>
                {% for content in all_content %}
                {% if 'missing-items' in content %}
                <p>
                    <a href="{{ content.document_link }}">{{ content.document_title }}</a><br>
                    {% if content.authors %}
                    {% for author in content.authors %}
                    <strong>Author:</strong> {{ author }}<br>
                    {% endfor %}
                    {% endif %}
                    <strong>Missing:</strong> {{ content['missing-items']|join(', ') }}
                </p>
                <p></p>
                {% endif %}
                {% endfor %}
            ''').render({k:v for k,v in vars().items() if k != 'self'})
            self.root.write_html(title, html)

        return good_content


    def download_assets(self):
        '''
        Download all top-level assets from top-level folders specified in config['asset-sources']
        '''
        # In case somebody uses this feature to write to a production server :(
        clean_path = lambda p: os.path.normpath(p.replace('\0','').replace('..','').strip('/'))

        destination = clean_path(self.config['asset-path'])
        with script_subdirectory(destination):
            for source in self.config['asset-sources']:
                folder = self.root.folder(source)
                for file in folder.files:
                    if file.save_as(file.title):
                        log('download: asset "{}"'.format(file.title))


if __name__ == '__main__':
    with only_one_process():
        ContentLoader()

