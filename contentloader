#!/usr/bin/env python3

# Auto-install and activate a virtualenv if possible
import autovenv
autovenv.run()

import argparse
import datetime
import os
import re
import sys
from itertools import zip_longest
from pprint import pprint

import archieml
import couchdb
import driveclient
import jinja2
import langdetect
from fuzzywuzzy.process import extractOne as fuzzy

from utils import *


DB_NAME = 'toolbox'
CONFIG_FILE_NAME = 'CONFIG'
ROOT_FOLDER_NAME = 'BR CONTENT'
DRIVE_CLIENT_NAME = 'beautiful-rising'
SERVICE_ACCOUNT_JSON_FILENAME = 'beautiful-rising-eae5ff71ae04.json'


class ContentLoader(object):
    def __init__(self):
        # Parse command line arguments
        arg_parser = argparse.ArgumentParser(description="")
        arg_parser_group = arg_parser.add_mutually_exclusive_group()
        arg_parser_group.add_argument('--id', type=str, metavar='DOCUMENT_ID', action='append', 
            default=[], dest='ids', help="Fetch a single document by its globally unique id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--change-id', type=str, metavar='CHANGE_ID', action='append', 
            default=[], dest='changes', help="Fetch a single document by its ephemeral change id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--watch-docs', action='store_true',
            help="Initiate a request to watch drive for changes. It will expire in one day.")
        arg_parser_group.add_argument('--stop-watching', action='store_true', 
            help="If the db has any record of a watch request, request that it be cancelled.")
        arg_parser_group.add_argument('--report-broken-docs', action='store_true', 
            help="Produce a document containing information about documents which lack required "
            "fields for their type, as specified in the config document.")
        arg_parser_group.add_argument('--delete-db', action='store_true',
            help='Delete any existing database named "{}".'.format(DB_NAME))
        self.options,_ = arg_parser.parse_known_args()

        # Connect to couchdb
        self.couch = couchdb.Server()
        self.db = self.couch[DB_NAME] if DB_NAME in self.couch else self.couch.create(DB_NAME)

        # Connect to Google Drive and get the root folder
        self.drive = driveclient.DriveClient(DRIVE_CLIENT_NAME,
            scopes='https://www.googleapis.com/auth/drive',
            service_account_json_filename=SERVICE_ACCOUNT_JSON_FILENAME)
        self.root = self.drive.folder(ROOT_FOLDER_NAME) 
        if not self.root:
            die("Can't find the root folder!")

        # Load CONFIG_FILE_NAME from ROOT_FOLDER_NAME and store as "config:api" & self.config
        self.configure()

        # Watch for changes
        if self.options.watch_docs:
            self.unwatch()
            self.watch()

        # Stop watching for changes
        elif self.options.stop_watching:
            self.unwatch()

        # Delete the database
        elif self.options.delete_db:
            confirm = input('Delete the database "{}" [y/N]? '.format(DB_NAME))
            if confirm.lower() == 'y':
                self.unwatch()
                del self.couch[DB_NAME]

        # Produce a report in the root folder about documents which lack required fields
        elif self.options.report_broken_docs:
            documents = self.get_documents()
            all_content = [c for c in map(self.extract_and_transform, documents) if c]
            all_content = self.merge_translations(all_content)
            all_content = self.filter_broken_content(all_content, report=True)

        # Load content
        else:
            #TODO: in production always unwatch/watch
            #TODO: handle document renaming/deletion

            # Fetch all the existing content from the db
            existing_content = {d.doc['slug']: d.doc for d in self.db.view('_all_docs', include_docs=True) 
                                if 'document-id' in d.doc}

            # Identify published documents by their filenames and fetch new content
            published = re.compile(self.config['published-filename-regex']).search
            published_documents = (d for d in self.get_documents() if published(d.title))
            new_content = filter(None, map(self.extract_and_transform, published_documents))

            # Merge new content with existing, preserving revision number and translations
            for content in new_content:
                existing = existing_content.get(content['slug'])
                if existing:
                    content['_rev'] = existing['_rev']
                    content['translations'] = existing.get('translations', {})
                existing_content[content['slug']] = content

            all_content = list(existing_content.values())
            all_content = self.merge_translations(all_content)
            all_content = self.filter_broken_content(all_content)
            all_content = self.download_assets(all_content)
            self.db_save(all_content)


    def type_info(self, content):
        '''
        Return the type data structure for the content item
        '''
        return next((T for T in self.config['types'] if T['name'] in content), {})


    def find_module(self, module_or_name, module_list, thresh=0.3):
        '''
        Use fuzzy matching to find a module from a list
        '''
        module_name = module_or_name
        if not isinstance(module_or_name, str):
            module_name = module_or_name['title']
        match = fuzzy(module_name, module_list, processor=lambda m: m['title'])
        if match and match[1] > thresh:
            return match[0]


    def db_save(self, doc_or_docs, id=None):
        '''
        Write one or many dicts (docs) to couchdb
        '''
        # Handle one or many docs
        docs,id = ([doc_or_docs], id) if isinstance(doc_or_docs, dict) else (doc_or_docs, None)
        # Remove couch-disallowed keys and add _id where needed
        docs = [{k:v for k,v in d.items() if k in ('_id', '_rev') or not k.startswith('_')} for d in docs]
        [d.update(_id=id or '{type}:{slug}'.format(**d)) for d in docs if '_id' not in d]
        # Simple conflict resolution (WARNING: this won't work with replication!)
        for success,id,rev_or_exc in self.db.update(docs):
            if isinstance(rev_or_exc, couchdb.http.ResourceConflict):
                retry = [d for d in docs if d['_id'] == id][0]
                retry.update(_rev=self.db[id]['_rev'])
                self.db.save(retry)


    def configure(self):
        '''
        Fetch, parse, set defaults, and store the config
        '''
        document = self.root.file(CONFIG_FILE_NAME)
        if not document:
            die("Can't find a config file!")

        self.config = c = archieml.loads(document.text)

        # Hiding configuration options from the API
        c.setdefault('config-hidden-keys', ['notification-token'])

        # Drive notifications
        c.setdefault('notification-url', '')
        c.setdefault('notification-token', '')

        # Language settings
        c.setdefault('language-default', 'en')
        c.setdefault('language-all', ['en'])
        c.setdefault('language-add', '')
        c.setdefault('language-remove', '')

        # How to distinguish published content
        c.setdefault('published-filename-regex', r'(?i)\bdone$')

        # Renaming synonymous keys, including those with language-suffixes
        c.setdefault('synonyms', {})

        # Shuffling content together
        c.setdefault('plural-separator-regex', r'(?:\s*,|\s+and|\s+&)\s+')
        c.setdefault('plural-keys', {})
        c.setdefault('plural-keys-collate', {})

        # Relationships between content items
        c.setdefault('relationships', {})

        # Associate assets files with a key and download them
        c.setdefault('assets', [{'key': 'image', 'source': 'images', 'destination': 'images'}])

        # Content type information
        c['types'] = []
        for key,value in c.items():
            if key.startswith('types-'):
                for type in value:
                    type['name'] = slugify(type['one'])
                c['types'] += value

        # Save the config before creating lots of temporary language-related data within it
        self.db_save(c, 'config:api')

        # Key transformations have to take into account language suffixes, so this adds suffixed copies
        # of synonyms, plural-keys, and plural-keys-collate (where each entry is a nested list).
        add_language_suffixes = lambda D: [D.update(each) for each in
            [{k+'-'+lang: [i+'-'+lang for i in v] if isinstance(v, list) else
                          v+'-'+lang for k,v in D.items()} for lang in self.config['language-all']] ]
        add_language_suffixes(c['synonyms'])
        add_language_suffixes(c['plural-keys'])
        add_language_suffixes(c['plural-keys-collate'])

        log('load: configuration options from drive document "{}"'.format(CONFIG_FILE_NAME))


    def watch(self):
        '''
        Request push notifications for entire drive be sent to the notification-url
        '''
        now = datetime.datetime.utcnow()
        expiration = int((60*60*24 + now.timestamp()) * 1000) # UTC + 24h in ms
        self.drive.execute(self.drive.service.changes().watch(body={
            'id': '{}-{}'.format(DRIVE_CLIENT_NAME, expiration),
            'type': 'web_hook',
            'address': self.config['notification-url'],
            'token': self.config['notification-token'],
            'expiration': expiration,
        }))
        log("watch: for push notifications at {}".format(self.config['notification-url']))
    
    
    def unwatch(self):
        '''
        Request all push notification channels in db be cancelled
        '''
        if 'config:notification-channels' in self.db:
            for channel,resource in self.db['config:notification-channels'].items():
                if channel not in ('_id', '_rev'):
                    try:
                        self.drive.execute(self.drive.service.channels().stop(body={
                            'id': channel, 
                            'resourceId': resource,
                        }))
                        log("stop: Channel-Id: {} Resource-Id: {}".format(channel, resource))
                    except: pass
            del self.db['config:notification-channels']


    def get_documents(self):
        '''
        Get documents by file ids, change ids or all documents
        '''
        documents = []
        # Get only the specifically requested documents by id or change id
        if self.options.ids or self.options.changes:
            documents.extend(d for d in (self.drive.get(id) for id in self.options.ids) if d)
            documents.extend(d for d in (self.drive.get_change(id) for id in self.options.changes) if d)
        # Get all documents
        else:
            # Recursive folder getter requires python3.3+ for "yield from"
            def get_folders(root):
                for folder in root.folders:
                    yield folder
                    yield from get_folders(folder)
            for folder in get_folders(self.root):
                documents.extend(folder.documents)
                log('find: content in drive folder "{}"'.format(folder.title))
        return documents


    def extract_and_transform(self, document):
        '''
        Process a document and return a content item.
        '''
        text = document.text
        text = text.replace('\r', '')
        # Strip google's comment annotations if at all possible
        text = re.sub(r'(?:\[([a-z])\1?\])+$|^\[([a-z])\2?\].+$', '', text, flags=re.M) 
        content = archieml.loads(text)

        # Rename synonymous keys (this should happen before all other transformations)
        for old_key,new_key in self.config['synonyms'].items():
            old_value = content.get(old_key)
            if old_value is not None:
                content[new_key] = old_value
                del content[old_key]

        # Determine the type
        type = self.type_info(content).get('name', '')
        content['type'] = type
        content['title'] = title = content.get(type, '')
        if not isinstance(title, str): 
            log("skip: {} bad type information".format(document.id))
            return

        # Add a few useful bits
        content['slug'] = slugify(content.get('title', ''), allow=':')
        content['document-id'] = document.id
        content['document-link'] = document.alternateLink
        content['document-title'] = document.title

        # Convert singular keys to plural keys and split them up as lists
        for plural_key,singular_key in self.config['plural-keys'].items():
            single, plural = content.get(singular_key), content.get(plural_key)
            if single:
                content[plural_key] = [single]
                if plural_key != singular_key:
                    del content[singular_key]
            if plural:
                multiline = re.split(r'\s*\n\s*\n\s*', plural)
                content[plural_key] = (multiline if len(multiline) > 1 else 
                                       re.split(self.config['plural-separator-regex'], plural))

        # Collate (zip) specific plural data together and give it new names
        for output,plurals in self.config['plural-keys-collate'].items():
            collated_lists = zip_longest(*(content.get(existing_key, []) for existing_key in plurals), fillvalue='')
            # If the output key has a language suffix, remove it from the inner keys
            if re.search('-(?:{})$'.format('|'.join(self.config['language-all'])), output):
                sans_suffix = [self.config['plural-keys'][p][:-3] for p in plurals] 
                collated_dicts = [dict(zip(sans_suffix, L)) for L in collated_lists]
            # Otherwise create the inner keys normally
            else:
                collated_dicts = [dict(zip(map(self.config['plural-keys'].get, plurals), L)) for L in collated_lists]
            for old_key in plurals:
                content.pop(old_key, None)
            if collated_dicts:
                content[output] = collated_dicts
        
        log("extract: {} ({}: {})".format(document.id, type, content['title']))
        return content


    def add_language_tag(self, content):
        '''
        Add language tag to a content item
        '''
        if 'lang' not in content:
            language_suffixes = ['-'+lang for lang in self.config['language-all']]
            text = [v for k,v in content.items() if isinstance(v, str) and k[-3:] not in language_suffixes]
            corpus = '\n'.join(sorted(text, key=len, reverse=True))
            try:
                lang = langdetect.detect(corpus)
                assert lang in self.config['language-all']
            except:
                lang = self.config['language-default']
            content['lang'] = lang


    def merge_translations(self, all_content):
        '''
        Assuming all necessary documents have already been fetched, merge translations
        into the content object for the default language. They will be placed into a 
        'translations' dictionary under two-letter language code keys.
        '''
        content_primary = []
        content_translated = []

        # Sort content by language
        for content in all_content:
            self.add_language_tag(content)
            if content['lang'] == self.config['language-default']:
                content_primary.append(content)
            else:
                content_translated.append(content)

        # Add translated content to a translations dict in each original language piece
        for translation in content_translated:
            original = self.find_module(translation.get('original', ''), content_primary)
            if not original:
                log("skip: {} can't find original {}".format(translation.get('title'), translation.get('original')))
                continue
            translations = original.setdefault('translations', {})
            translations[translation['lang']] = translation
            log("merge: {} ({}) => {}".format(translation['title'], translation['lang'], original['title']))

        # Look through primary language documents and move translated keys into place
        for module in content_primary:
            module.setdefault('translations', {})
            for lang in self.config['language-all']:
                keys = [k for k in module if isinstance(k, str) and k.endswith('-'+lang)]
                inline = {k[:-3]: module[k] for k in keys if module[k]}
                if inline:
                    if lang == self.config['language-default']:
                        module.update(inline)
                    else:
                        lang_dict = module['translations'].setdefault(lang, {})
                        lang_dict.update(inline)
                    [module.pop(k) for k in keys]
                    print(lang, '-=>', inline) #TODO: remove

        #TODO: descend into each translation and pull keys back out for other languages?
        return content_primary


    def filter_broken_content(self, all_content, report=False):
        '''
        Remove content with obvious content problems from all_content
        '''
        for content in all_content:
            required_sets = [r.split('|') for r in self.type_info(content).get('required', [])]
            missing_items = [' or '.join(s) for s in required_sets if not any(content.get(k) for k in s)]
            if missing_items:
                content['missing-items'] = missing_items
                log('missing:', content['title'], missing_items)
        good_content = [c for c in all_content if 'missing-items' not in c]

        if report:
            title = 'Content Error Report'
            timestamp = datetime.datetime.utcnow()
            html = jinja2.Template('''
                <style>* { font-family: "Consolas"; }</style>
                <h1>{{ title }}</h1>
                <p>
                    Modifications to this file will be discarded.<br>
                    Translated pieces are not currently represented in this report.<br>
                    <em>Updated: {{ timestamp }} UTC</em><br>
                </p>
                <hr>
                {% for content in all_content %}
                {% if 'missing-items' in content %}
                <p>
                    <a href="{{ content['document-link'] }}">{{ content['document-title'] }}</a><br>
                    {% if content.authors %}
                    {% for author in content.authors %}
                    <strong>Author:</strong> {{ author }}<br>
                    {% endfor %}
                    {% endif %}
                    <strong>Missing:</strong> {{ content['missing-items']|join(', ') }}
                </p>
                <p></p>
                {% endif %}
                {% endfor %}
            ''').render({k:v for k,v in vars().items() if k != 'self'})
            self.root.write_html(title, html)

        return good_content


    def download_assets(self, all_content):
        '''
        This modifies all_content and should be called before the final db store.
        '''
        # In case somebody uses this feature to write to a production server :(
        clean_path = lambda p,base='assets': os.path.normpath(os.path.join(base,
            p.replace('\0','').replace('..','').strip('/')))

        # TODO: add image scaling, etc
        for asset_spec in self.config['assets']:
            asset_folder = self.root.folder(asset_spec['source'])
            if not asset_folder:
                continue
            asset_key = asset_spec['key']
            with script_subdirectory(clean_path(asset_spec['destination'])):
                # Iterate first over drive files because it's slow
                for asset in asset_folder.files:
                    asset_name = os.path.splitext(asset.title.lower())[0]
                    # Then find content items which require the asset
                    for content in all_content:
                        desired_asset = os.path.splitext(content.get(asset_key, '').lower())[0]
                        if (desired_asset == asset_name):
                            # Slugify and update the filename
                            name, ext = os.path.splitext(asset.title)
                            filename = slugify(name) + ext
                            content[asset_key] = filename
                            # Download the actual file
                            if asset.save_as(filename):
                                log('download: asset "{}"'.format(filename))
        return all_content


if __name__ == '__main__':
    with only_one_process():
        ContentLoader()

