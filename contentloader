#!/usr/bin/env python3

# Auto-install and activate a virtualenv if possible
import autovenv
autovenv.run()

import argparse
import datetime
import re
from io import BytesIO
from itertools import zip_longest

import archieml
import couchdb
import driveclient
import jinja2
import langdetect
from fuzzywuzzy.process import extractOne as fuzzy
from apiclient.http import MediaIoBaseUpload

from utils import *


DB_NAME = 'toolbox'
CONFIG_FILE_NAME = 'CONFIG'
ROOT_FOLDER_NAME = 'BR CONTENT'
DRIVE_CLIENT_NAME = 'beautiful-rising'
SERVICE_ACCOUNT_JSON_FILENAME = 'beautiful-rising-eae5ff71ae04.json'


class ContentLoader(object):
    def __init__(self):
        # Parse command line arguments
        argparser = argparse.ArgumentParser(description="")
        argparser.add_argument('--id', type=str, metavar='DOCUMENT_ID', action='append', 
            default=[], dest='ids', help="Fetch a single document by its globally unique id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once."
        )
        argparser.add_argument('--change-id', type=str, metavar='CHANGE_ID', action='append', 
            default=[], dest='changes', help="Fetch a single document by its ephemeral change id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once."
        )
        argparser.add_argument('--watch-docs', action='store_true',
            help="Initiate a request to watch drive for changes. It will expire in one day."
        )
        argparser.add_argument('--stop-watching', action='store_true', 
            help="If the db has any record of a watch request, request that it be cancelled."
        )
        argparser.add_argument('--report-broken-docs', action='store_true', 
            help="Produce a document containing information about documents which lack required "
            "fields for their type, as specified in the config document."
        )
        argparser.add_argument('--delete-db', action='store_true',
            help='Delete any existing database named "{}".'.format(DB_NAME)
        )
        self.options,_ = argparser.parse_known_args()

        # Connect to couchdb
        self.couch = couchdb.Server()
        self.db = self.couch[DB_NAME] if DB_NAME in self.couch else self.couch.create(DB_NAME)

        # Connect to Google Drive and get the root folder
        self.drive = driveclient.DriveClient(DRIVE_CLIENT_NAME,
            scopes='https://www.googleapis.com/auth/drive',
            service_account_json_filename=SERVICE_ACCOUNT_JSON_FILENAME)
        self.root = self.drive.folder(ROOT_FOLDER_NAME) 
        if not self.root:
            die("Can't find the root folder!")

        # Load CONFIG_FILE_NAME from ROOT_FOLDER_NAME and store as "config:api" & self.config
        self.configure()

        # Watch for changes
        if self.options.watch_docs:
            self.unwatch()
            self.watch()

        # Stop watching for changes
        elif self.options.stop_watching:
            self.unwatch()

        # Delete the database
        elif self.options.delete_db:
            confirm = input('Delete the database "{}" [y/N]? '.format(DB_NAME))
            if confirm.lower() == 'y':
                self.unwatch()
                del self.couch[DB_NAME]

        # Produce a report in the root folder about documents which lack required fields
        elif self.options.report_broken_docs:
            documents = self.get_documents()
            all_content = [c for c in map(self.extract_and_transform, documents) if c]
            all_content = self.remove_broken_content(all_content, report=True)

        # Load content
        else:
            #TODO: handle document renaming/deletion
            #TODO: in production always unwatch/watch
            is_published = lambda doc: re.search(self.config['published-filename-regex'], doc.title)

            documents = filter(is_published, self.get_documents())
            all_content = [c for c in map(self.extract_and_transform, documents) if c]

            #TODO: ensure translated docs get fetched (from db) so merges can happen
            all_content = self.merge_translations(all_content)

            all_content = self.remove_broken_content(all_content)
            self.download_assets(all_content)

            for content in all_content:
                self.db_save(content)


    def content_id(self, content):
        '''
        Produce consistent ids for couchdb
        '''
        return '{type}:{slug}'.format(**content)


    def type_info(self, content):
        '''
        Return the type data structure for the content item
        '''
        return next((T for T in self.config['types'] if T['name'] in content), {})


    def find_module(self, module_or_name, module_list, thresh=0.3):
        '''
        Use fuzzy matching to find a module from a list
        '''
        module_name = module_or_name
        if not isinstance(module_or_name, str):
            module_name = module_or_name['title']
        match, score = fuzzy(module_name, module_list, processor=lambda m: m['title'])
        return match if score > thresh else None


    def db_save(self, doc, id=None):
        '''
        Write doc to couchdb, adding a revision if the document exists
        To clarify, this is a couchdb doc and not a google doc.
        '''
        doc = {k:v for k,v in doc.items() if not k.startswith('_')}
        doc.update(_id=id or self.content_id(doc))
        try:
            doc.update(_rev=self.db[doc['_id']]['_rev'])
        except couchdb.http.ResourceNotFound: pass
        self.db.save(doc)


    def configure(self):
        '''
        Fetch, parse, set defaults, and store the config
        '''
        document = self.root.file(CONFIG_FILE_NAME)
        if not document:
            die("Can't find a config file!")

        self.config = c = archieml.loads(document.text)

        # Hiding configuration options from the API
        c.setdefault('config-hidden-keys', ['notification-token'])

        # Drive notifications
        c.setdefault('notification-url', '')
        c.setdefault('notification-token', '')

        # Language settings
        c.setdefault('language-default', 'en')
        c.setdefault('language-all', ['en'])
        c.setdefault('language-add', '')
        c.setdefault('language-remove', '')

        # How to distinguish published content
        c.setdefault('published-filename-regex', r'(?i)\bdone$')

        # Renaming synonymous keys, including those with language-suffixes
        c.setdefault('synonyms', {})

        # Shuffling content together
        c.setdefault('plural-separator-regex', r'(?:\s*,|\s+and|\s+&)\s+')
        c.setdefault('plural-keys', {})
        c.setdefault('plural-keys-collate', {})

        # Associate assets files with a key and download them
        c.setdefault('assets', [{'key': 'image', 'source': 'images', 'destination': 'images'}])

        # Content type information
        c['types'] = []
        for key,value in c.items():
            if key.startswith('types-'):
                for type in value:
                    type['name'] = slugify(type['one'])
                c['types'] += value

        # Save the config before creating lots of temporary language-related data within it
        self.db_save(c, 'config:api')

        # Key transformations have to take into account language suffixes, so this adds suffixed copies
        # of synonyms, plural-keys, and plural-keys-collate (where each entry is a nested list).
        add_language_suffixes = lambda D: [D.update(each) for each in
            [{k+'-'+lang: [i+'-'+lang for i in v] if isinstance(v, list) else
                          v+'-'+lang for k,v in D.items()} for lang in self.config['language-all']] ]
        add_language_suffixes(c['synonyms'])
        add_language_suffixes(c['plural-keys'])
        add_language_suffixes(c['plural-keys-collate'])

        log('load: configuration options from drive document "{}"'.format(CONFIG_FILE_NAME))


    def watch(self):
        '''
        Request push notifications for entire drive be sent to the notification-url
        '''
        now = datetime.datetime.utcnow()
        expiration = int((60*60*24 + now.timestamp()) * 1000) # UTC + 24h in ms
        self.drive.execute(self.drive.service.changes().watch(body={
            'id': '{}-{}'.format(DRIVE_CLIENT_NAME, expiration),
            'type': 'web_hook',
            'address': self.config['notification-url'],
            'token': self.config['notification-token'],
            'expiration': expiration,
        }))
        log("watch: for push notifications at {}".format(self.config['notification-url']))
    
    
    def unwatch(self):
        '''
        Request all push notification channels in db be cancelled
        '''
        if 'config:notification-channels' in self.db:
            for channel,resource in self.db['config:notification-channels'].items():
                if channel not in ('_id', '_rev'):
                    try:
                        self.drive.execute(self.drive.service.channels().stop(body={
                            'id': channel, 
                            'resourceId': resource,
                        }))
                        log("stop: Channel-Id: {} Resource-Id: {}".format(channel, resource))
                    except: pass
            del self.db['config:notification-channels']


    def get_documents(self):
        '''
        Get documents by file ids, change ids or all documents
        '''
        documents = []
        # Get only the specifically requested documents by id or change id
        if self.options.changes or self.options.ids:
            documents.extend(d for d in (self.drive.get(id) for id in self.options.ids) if d)
            documents.extend(d for d in (self.drive.get_change(id) for id in self.options.changes) if d)
        # Get all documents
        else:
            # Recursive folder getter requires python3.3+ for "yield from"
            def get_folders(root):
                for folder in root.folders:
                    yield folder
                    yield from get_folders(folder)
            for folder in get_folders(self.root):
                documents.extend(folder.documents)
                log('find: content in drive folder "{}"'.format(folder.title))
        return documents


    def extract_and_transform(self, document):
        '''
        Process a document and return a content item.
        '''
        text = document.text
        text = text.replace('\r', '')
        # Strip google's comment annotations if at all possible
        text = re.sub(r'(?:\[([a-z])\1?\])+$|^\[([a-z])\2?\].+$', '', text, flags=re.M) 
        content = archieml.loads(text)

        # Rename synonymous keys (this should happen before all other transformations)
        for old_key,new_key in self.config['synonyms'].items():
            old_value = content.get(old_key)
            if old_value is not None:
                content[new_key] = old_value
                del content[old_key]

        # Determine the type
        type = self.type_info(content).get('name', '')
        content['type'] = type
        content['title'] = title = content.get(type, '')
        if not isinstance(title, str): 
            log("skip: {} bad type information".format(document.id))
            return

        # Add a few useful bits
        content['slug'] = slugify(content.get('title', ''), allow=':')
        content['document-id'] = document.id
        content['document-link'] = document.alternateLink
        content['document-title'] = document.title

        # Convert singular keys to plural keys and split them up as lists
        for plural_key,singular_key in self.config['plural-keys'].items():
            single, plural = content.get(singular_key), content.get(plural_key)
            if single:
                content[plural_key] = [single]
                if plural_key != singular_key:
                    del content[singular_key]
            if plural:
                multiline = re.split(r'\s*\n\s*\n\s*', plural)
                content[plural_key] = (multiline if len(multiline) > 1 else 
                                       re.split(self.config['plural-separator-regex'], plural))

        # Collate (zip) specific plural data together and give it new names
        for output,plurals in self.config['plural-keys-collate'].items():
            collated_lists = zip_longest(*(content.get(existing_key, []) for existing_key in plurals), fillvalue='')
            # If the output key has a language suffix, remove it from the inner keys
            if re.search('-(?:{})$'.format('|'.join(self.config['language-all'])), output):
                sans_suffix = [self.config['plural-keys'][p][:-3] for p in plurals] 
                collated_dicts = [dict(zip(sans_suffix, L)) for L in collated_lists]
            # Otherwise create the inner keys normally
            else:
                collated_dicts = [dict(zip(map(self.config['plural-keys'].get, plurals), L)) for L in collated_lists]
            for old_key in plurals:
                content.pop(old_key, None)
            if collated_dicts:
                content[output] = collated_dicts
        
        log("extract: {} ({}: {})".format(document.id, type, content['title']))
        return content


    def add_language_tag(self, content):
        '''
        Add language tag to a content item
        '''
        language_suffixes = ['-'+lang for lang in self.config['language-all']]
        if 'lang' not in content:
            text = [v for k,v in content.items() if isinstance(v, str) and k[-3:] not in language_suffixes]
            corpus = '\n'.join(sorted(text, key=len, reverse=True))
            try:
                lang = langdetect.detect(corpus)
                assert lang in self.config['language-all']
            except:
                lang = self.config['language-default']
            content['lang'] = lang


    def merge_translations(self, all_content):
        '''
        Assuming all necessary documents have already been fetched, merge translations
        into the content object for the default language. They will be placed into a 
        'translations' dictionary under two-letter language code keys.
        '''
        content_primary = []
        content_translated = []

        # Sort content by language
        for content in all_content:
            self.add_language_tag(content)
            if content['lang'] == self.config['language-default']:
                content_primary.append(content)
            else:
                content_translated.append(content)

        # Add translated content to a translations dict in each original language piece
        for translation in content_translated:
            original = self.find_module(translation.get('original', ''), content_primary)
            if not original:
                continue
            translations = original.setdefault('translations', {})
            translations[translation['lang']] = translation
            log("merge: {} ({}) => {}".format(translation['title'], translation['lang'], original['title']))

        # Look through primary language documents and move translated keys into place
        for module in content_primary:
            module.setdefault('translations', {})
            for lang in self.config['language-all']:
                keys = [k for k in module if k.endswith('-'+lang)]
                inline = {k[:-3]: module[k] for k in keys}
                if inline:
                    if lang == self.config['language-default']:
                        module.update(inline)
                    else:
                        lang_dict = module['translations'].setdefault(lang, {})
                        lang_dict.update(inline)
                    [module.pop(k) for k in keys]
                #TODO: remove
                print(lang, '-=>', inline)

        #TODO: descend into each translation and pull keys back out for other languages?
        return content_primary


    def remove_broken_content(self, all_content, report=False):
        '''
        Remove content with obvious content problems from all_content
        '''
        for content in all_content:
            required_sets = [r.split('|') for r in self.type_info(content).get('required', [])]
            missing_items = [' or '.join(s) for s in required_sets if not any(content.get(k) for k in s)]
            if missing_items:
                content['missing-items'] = missing_items
                log('remove:', content['title'], missing_items)
        good_content = [c for c in all_content if 'missing-items' not in c]

        if report:
            title = 'Content Error Report'
            timestamp = datetime.datetime.utcnow()
            html = jinja2.Template('''
                <style>* { font-family: "Consolas"; }</style>
                <h1>{{ title }}</h1>
                <p>
                    Modifications to this file will be discarded.<br>
                    Please disregard missing keys in translated pieces for now.<br>
                    <em>Updated: {{ timestamp }} UTC</em><br>
                </p>
                <hr>
                {% for content in all_content %}
                {% if 'missing-items' in content %}
                <p>
                    <a href="{{ content['document-link'] }}">{{ content['document-title'] }}</a><br>
                    {% if content.authors %}
                    {% for author in content.authors %}
                    <strong>Author:</strong> {{ author }}<br>
                    {% endfor %}
                    {% endif %}
                    <strong>Missing:</strong> {{ content['missing-items']|join(', ') }}
                </p>
                <p></p>
                {% endif %}
                {% endfor %}
            ''').render({k:v for k,v in vars().items() if k != 'self'})
            self.root.write_html(title, html)

        return good_content


    def download_assets(self, all_content):
        '''
        This modifies all_content and should be called before the final db store.
        '''
        for asset_spec in self.config['assets']:
            asset_folder = self.root.folder(asset_spec['source'])
            if not asset_folder:
                continue
            with script_subdirectory(asset_spec['destination']):
                # Iterate first over drive files because it's slow
                for asset in asset_folder.files:
                    # Find content items which require the asset
                    for content in all_content:
                        asset_name = content.get(asset_spec['key'])
                        if asset_name == asset.title or asset_name == asset.title.rsplit('.')[0]:
                            # Update the filename
                            content[asset_spec['key']] = asset.title
                            # TODO: Make this smart enough to prevent re-downloading
                            #asset.save_as(asset.title)
                            log('download: asset "{}"'.format(asset.title))


if __name__ == '__main__':
    ContentLoader()

