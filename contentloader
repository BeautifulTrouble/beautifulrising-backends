#!/usr/bin/env python3

# Auto-install and activate a virtualenv if possible
import autovenv
autovenv.run()

import argparse
import datetime
import re
from io import BytesIO
from itertools import zip_longest

import archieml
import couchdb
import driveclient
import jinja2
import langdetect
from fuzzywuzzy.process import extractOne as fuzzy
from apiclient.http import MediaIoBaseUpload

from utils import *


DB_NAME = 'toolbox'
CONFIG_FILE_NAME = 'CONFIG'
ROOT_FOLDER_NAME = 'BR CONTENT'
DRIVE_CLIENT_NAME = 'beautiful-rising'
SERVICE_ACCOUNT_JSON_FILENAME = 'beautiful-rising-eae5ff71ae04.json'


class ContentLoader(object):
    def __init__(self):
        # Parse command line arguments
        argparser = argparse.ArgumentParser(description="")
        argparser.add_argument('--id', type=str, metavar='DOCUMENT_ID', action='append', 
            default=[], dest='ids', help="Fetch a single document by its globally unique id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once."
        )
        argparser.add_argument('--change-id', type=str, metavar='CHANGE_ID', action='append', 
            default=[], dest='changes', help="Fetch a single document by its ephemeral change id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once."
        )
        argparser.add_argument('--watch-docs', action='store_true',
            help="Initiate a request to watch drive for changes. It will expire in one day."
        )
        argparser.add_argument('--stop-watching', action='store_true', 
            help="If the db has any record of a watch request, request that it be cancelled."
        )
        argparser.add_argument('--report-broken-docs', action='store_true', 
            help="Produce a document containing information about documents which lack required "
            "fields for their type, as specified in the config document."
        )
        argparser.add_argument('--delete-db', action='store_true',
            help='Delete any existing database named "{}".'.format(DB_NAME)
        )
        self.options,_ = argparser.parse_known_args()

        # Connect to couchdb
        self.couch = couchdb.Server()
        self.db = self.couch[DB_NAME] if DB_NAME in self.couch else self.couch.create(DB_NAME)

        # Connect to Google Drive and get the root folder
        self.drive = driveclient.DriveClient(DRIVE_CLIENT_NAME,
            scopes='https://www.googleapis.com/auth/drive',
            service_account_json_filename=SERVICE_ACCOUNT_JSON_FILENAME)
        self.root = self.drive.folder(ROOT_FOLDER_NAME) 
        if not self.root:
            die("Can't find the root folder!")

        # Master lists of content
        self.all_content = []
        self.published_content = []

        # Load CONFIG_FILE_NAME from ROOT_FOLDER_NAME and store as "config:api" & self.config
        self.configure()

        # -------------------------------------------
        # Decide behavior based on command line args
        # -------------------------------------------

        # Watch for changes
        if self.options.watch_docs:
            self.unwatch()
            self.watch()

        # Stop watching for changes
        elif self.options.stop_watching:
            self.unwatch()

        # Delete the database
        elif self.options.delete_db:
            confirm = input('Delete the database "{}" [y/N]? '.format(DB_NAME))
            if confirm.lower() == 'y':
                self.unwatch()
                del self.couch[DB_NAME]

        elif self.options.report_broken_docs:
            #XXX: keep this separate from the real loader, as the lord intended
            pass

        # Load content
        else:
            #TODO: handle document renaming/deletion
            documents = self.get_documents()

            for document in documents:
                published = re.search(self.config['published-keyword'], document.title)
                if published or self.options.report_broken_docs:
                    content = self.extract_and_transform(document)
                    self.all_content.append(content)
                    if published:
                        self.published_content.append(content)

            self.filter_broken_documents()
            self.download_assets()
            for content in self.published_content:
                self.db_save(content)


    def content_id(self, content):
        '''
        Produce consistent ids for couchdb
        '''
        return '{type}:{slug}'.format(**content)


    def type_info(self, content):
        '''
        Return the type data structure for the content item
        '''
        return next((T for T in self.config['types'] if T['name'] in content), {})


    def find_module(self, module_or_name, module_list):
        '''
        Use fuzzy matching to find a module from a list
        '''
        module_name = module_or_name
        if not isinstance(module_or_name, str):
            module_name = module_or_name['title']
        match, score = fuzzy(module_name, module_list, processor=lambda m: m['title'])
        return match


    def db_save(self, doc, id=None):
        '''
        Write doc to couchdb, adding a revision if the document exists
        To clarify, this is a couchdb doc and not a google doc.
        '''
        doc = {k:v for k,v in doc.items() if not k.startswith('_')}
        doc.update(_id=id or self.content_id(doc))
        try:
            doc.update(_rev=self.db[doc['_id']]['_rev'])
        except couchdb.http.ResourceNotFound: pass
        self.db.save(doc)


    def configure(self):
        '''
        Fetch, parse, set defaults, and store the config
        '''
        document = self.root.file(CONFIG_FILE_NAME)
        if not document:
            die("Can't find a config file!")

        self.config = c = archieml.loads(document.text)

        # Hiding configuration options from the API
        c.setdefault('config-hidden-keys', ['notification-token'])

        # Drive notifications
        c.setdefault('notification-url', '')
        c.setdefault('notification-token', '')

        # Language settings
        c.setdefault('language-default', 'en')
        c.setdefault('language-all', ['en'])
        c.setdefault('language-add', '')
        c.setdefault('language-remove', '')

        # How to distinguish published content
        c.setdefault('published-filename-regex', r'(?i)\bdone$')

        # Renaming synonymous keys, including those with language-suffixes
        c.setdefault('synonyms', {})

        # Shuffling content together
        c.setdefault('plural-separator-regex', r'(?:\s*,|\s+and|\s+&)\s+')
        c.setdefault('plural-keys', {})
        c.setdefault('plural-keys-collate', {})

        # Associate assets files with a key and download them
        c.setdefault('assets', [{'key': 'image', 'source': 'images', 'destination': 'images'}])

        # Content type information
        c['types'] = []
        for key,value in c.items():
            if key.startswith('types-'):
                for type in value:
                    type['name'] = slugify(type['one'])
                c['types'] += value

        # Save the config before creating lots of temporary language-related data within it
        self.db_save(c, 'config:api')

        # Key transformations have to take into account language suffixes, so this adds suffixed copies
        # of synonyms, plural-keys, and plural-keys-collate (where each entry is a nested list).
        add_language_suffixes = lambda D: [D.update(each) for each in
            [{k+'-'+lang: [i+'-'+lang for i in v] if isinstance(v, list) else
                          v+'-'+lang for k,v in D.items()} for lang in self.config['language-all']] ]
        add_language_suffixes(c['synonyms'])
        add_language_suffixes(c['plural-keys'])
        add_language_suffixes(c['plural-keys-collate'])

        log('load: configuration options from drive document "{}"'.format(CONFIG_FILE_NAME))


    def watch(self):
        '''
        Request push notifications for entire drive be sent to the notification-url
        '''
        now = datetime.datetime.utcnow()
        expiration = int((60*60*24 + now.timestamp()) * 1000) # UTC + 24h in ms
        self.drive.execute(self.drive.service.changes().watch(body={
            'id': '{}-{}'.format(DRIVE_CLIENT_NAME, expiration),
            'type': 'web_hook',
            'address': self.config['notification-url'],
            'token': self.config['notification-token'],
            'expiration': expiration,
        }))
        log("watch: for push notifications at {}".format(self.config['notification-url']))
    
    
    def unwatch(self):
        '''
        Request all push notification channels in db be cancelled
        '''
        if 'config:notification-channels' in self.db:
            for channel,resource in self.db['config:notification-channels'].items():
                if channel not in ('_id', '_rev'):
                    try:
                        self.drive.execute(self.drive.service.channels().stop(body={'id': channel, 'resourceId': resource}))
                        log("stop: Channel-Id: {} Resource-Id: {}".format(channel, resource))
                    except: pass
            del self.db['config:notification-channels']


    def get_documents(self):
        '''
        Get content by file ids, change ids or all documents
        '''
        documents = []
        # Get only the requested documents
        if self.options.changes or self.options.ids:
            documents.extend(d for d in (self.drive.change(id) for id in self.options.changes) if d)
            documents.extend(d for d in (self.drive.get(id) for id in self.options.ids) if d)
        # Get all documents from the content folders
        else:
            for folder in self.root.folders:
                # Try to match one of the content types with this folder's name
                if any(re.search(pat, folder.title) for pat in self.config['content']):
                    log('find: content in drive folder "{}"'.format(folder.title))
                    documents.extend(folder.documents)
        return documents


    def extract_and_transform(self, document):
        '''
        Process a document and return a content item.
        '''
        text = document.text
        text = text.replace('\r', '')
        # Strip google's comment annotations if at all possible
        text = re.sub(r'(?:\[([a-z])\1?\])+$|^\[([a-z])\2?\].+$', '', text, flags=re.M) 
        content = archieml.loads(text)

        #TODO: Strip as much whitespace from values as possible

        # Rename synonymous keys (this should happen before all other transformations)
        for old_key,new_key in self.config['synonyms'].items():
            old_value = content.get(old_key)
            if old_value is not None:
                content[new_key] = old_value
                del content[old_key]

        # Figure out the content language
        if 'lang' not in content:
            content['lang'] = self.config['language-default']
            corpus = '\n'.join(sorted((s for s in content.values() if isinstance(s, str)), key=len, reverse=True))
            try:
                content['lang'] = langdetect.detect(corpus)
            except: pass

        # Add a few useful bits
        type = self.type_info(content).get('name', '')
        content['type'] = type
        content['title'] = content.get(type, '')
        content['slug'] = slugify(content['title'], allow=':')
        content['document_id'] = document.id
        content['document_link'] = document.alternateLink
        content['document_title'] = document.title

        # Convert singular keys to plural keys and split them up as lists
        for plural_key,singular_key in self.config['plural-keys'].items():
            single, plural = content.get(singular_key), content.get(plural_key)
            if single:
                content[plural_key] = [single]
                del content[singular_key]
            if plural:
                multiline = re.split(r'\s*\n\s*\n\s*', plural)
                content[plural_key] = (multiline if len(multiline) > 1 else 
                                       re.split(self.config['plural-separator'], plural))

        # Collate (zip) specific plural data together and give it new names
        for result,mapping in self.config['collate'].items():
            collated_lists = zip_longest(*(content.get(existing_key, []) 
                                for existing_key in mapping.values()), fillvalue='')
            collated_dicts = [dict(zip(mapping.keys(), L)) for L in collated_lists]
            for old_key in mapping.values():
                content.pop(old_key, None)
            content[result] = collated_dicts
        
        log("extract: {} ({}: {})".format(document.id, type, content['title']))
        return content


    def filter_broken_documents(self):
        '''
        Remove docs from published_content if they have obvious content problems 
        '''
        missing = []
        for content in self.all_content:
            missing_items = []
            required = re.split(self.config['plural-separator'], self.type_info(content).get('required', ''))
            for requirement in filter(None, required):
                if not any(content.get(field.strip()) for field in requirement.split('|')):
                    missing_items.append(requirement.replace('|', ' or '))
            if missing_items:
                missing.append((content, missing_items))
                if content in self.published_content:
                    self.published_content.remove(content)
        if self.options.report_broken_docs:
            self.report_broken_documents(missing)


    def download_assets(self):
        '''
        This modifies published_content and should be called before the final db store.
        '''
        for asset_spec in self.config['assets']:
            asset_folder = self.root.folder(asset_spec['source'])
            if not asset_folder:
                continue
            with script_subdirectory(asset_spec['destination']):
                # Iterate first over drive files because it's slow
                for asset in asset_folder.files:
                    # Find content items which require the asset
                    for content in self.published_content:
                        asset_name = content.get(asset_spec['key'])
                        if asset_name == asset.title or asset_name == asset.title.rsplit('.')[0]:
                            # Update the filename
                            content[asset_spec['key']] = asset.title
                            # TODO: Make this smart enough to prevent re-downloading
                            #asset.save_as(asset.title)
                            log('download: asset "{}"'.format(asset.title))


    def write_document(self, name, folder, bytestring, source_mimetype='text/plain'):
        '''
        Given a filename and a parent folder, create or update a document with a bytestring
        Docs will be converted from their source_mimetype
        '''
        params = {
            'body': {
                'title': name,
                'mimeType': 'application/vnd.google-apps.document',
            },
            'convert': True,
            'media_body': MediaIoBaseUpload(BytesIO(bytestring), mimetype=source_mimetype),
        }
        existing_file = folder.file(name)
        if existing_file:
            self.drive.execute(self.drive.service.files().update(fileId=existing_file.id, **params))
        else:
            params['body']['parents'] = [{'id': folder.id}]
            self.drive.execute(self.drive.service.files().insert(**params))


    def report_broken_documents(self, missing):
        report_name = 'Content Error Report'
        timestamp = datetime.datetime.utcnow()
        output = jinja2.Template('''
            <style>* { font-family: "Consolas"; }</style>
            <h1>{{ report_name }}</h1>
            <p>
                Modifications to this file will be discarded.<br><em>Updated: {{ timestamp }} UTC</em>
            </p>
            <hr>
            {% for content,missing_items in missing %}
            <p>
                <a href="{{ content.document_link }}">{{ content.document_title }}</a><br>
                {% if content.authors %}
                {% for author in content.authors %}
                <strong>Author:</strong> {{ author.name }} {{ author.email }}<br>
                {% endfor %}
                {% endif %}
                <strong>Missing:</strong> {{ missing_items|join(', ') }}
            </p><p></p>
            {% endfor %}
        ''').render({k:v for k,v in vars().items() if k != 'self'})
        self.write_document(report_name, self.root, output.encode('ascii', 'xmlcharrefreplace'), 'text/html')
                

if __name__ == '__main__':
    ContentLoader()

