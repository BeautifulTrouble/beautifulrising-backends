#!/usr/bin/env python3
#encoding: utf-8

# Auto-install and activate a virtualenv if possible
import autovenv
autovenv.run()

import argparse
import math
import os
import re
import shlex
import sys
import time
from copy import deepcopy
from datetime import datetime
from dateutil import parser
from hashlib import md5
from itertools import zip_longest
from pprint import pprint
from string import capwords
from subprocess import Popen
from urllib.parse import urljoin

import couchdb
import driveclient
import jinja2
import langdetect
import requests
from fuzzywuzzy.process import extractOne as fuzzy

from utils import *
from config import *


NEW = '_new_content'


class ContentLoader(object):
    def __init__(self):
        # Parse command line arguments
        arg_parser = argparse.ArgumentParser(description="")
        arg_parser_group = arg_parser.add_mutually_exclusive_group()
        arg_parser_group.add_argument('--id', type=str, metavar='DOCUMENT_ID', action='append', 
            default=[], dest='ids', help="Fetch a single document by its globally unique id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--change-id', type=str, metavar='CHANGE_ID', action='append', 
            default=[], dest='changes', help="Fetch a single document by its ephemeral change id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--assets', action='store_true', help="Download and convert "
            "all assets. Use after changing asset settings in the config.")
        arg_parser_group.add_argument('--watch-docs', action='store_true',
            help="Initiate a request to watch drive for changes. It will expire in one day.")
        arg_parser_group.add_argument('--stop-watching', action='store_true', 
            help="If the db has any record of a watch request, request that it be cancelled.")
        arg_parser_group.add_argument('--delete-db', action='store_true',
            help='Delete any existing database named "{}".'.format(DB_NAME))
        self.options,_ = arg_parser.parse_known_args()

        # Connect to couchdb
        self.couch = couchdb.Server(DB_SERVER)
        self.db_get_or_create()

        # Connect to Google Drive and get the root folder
        self.drive = driveclient.DriveClient(DRIVE_CLIENT_NAME,
            scopes='https://www.googleapis.com/auth/drive',
            service_account_json_filename=DRIVE_SERVICE_ACCOUNT_JSON_FILENAME)
        self.root = self.drive.folder(DRIVE_ROOT_FOLDER_NAME) 
        if not self.root:
            die("Can't find the root folder!")

        # Load DRIVE_CONFIG_FILE_NAME from DRIVE_ROOT_FOLDER_NAME and store as "config:api" & self.config
        self.configure()

        # Download assets
        if self.options.assets:
            self.download_assets(force_conversion=True)

        # Watch for changes
        elif self.options.watch_docs:
            self.unwatch()
            self.watch()

        # Stop watching for changes
        elif self.options.stop_watching:
            self.unwatch()

        # Delete the database
        elif self.options.delete_db:
            confirm = input('Delete the database "{}" [y/N]? '.format(DB_NAME))
            if confirm.lower() == 'y':
                self.unwatch()
                del self.couch[DB_NAME]

        # Load content
        else:
            #TODO: in production always unwatch/watch
            #TODO: handle document renaming/deletion

            # Fetch all the existing content from the db
            existing_content = {d.doc['slug']: d.doc for d in self.db.view('_all_docs', include_docs=True) 
                                if 'document_id' in d.doc}

            # Identify published documents by their filenames and fetch new content
            published = re.compile(self.config['published-filename-regex']).search
            published_documents = (d for d in self.get_documents() if published(d.title) or d.id in self.options.ids)
            new_content = filter(None, map(self.extract_and_transform, published_documents))

            # TODO: detect duplicate names

            # Merge new content with existing, preserving revision number and translations
            for content in new_content:
                existing = existing_content.get(content['slug'])
                #TODO: this is a bug and should be fixed (duplicate titles -> duplicate slugs -> duplicate ids -> incorrect reasoning about what is or isn't an existing piece)
                if existing and '_rev' in existing: 
                    content['_rev'] = existing['_rev']
                    content['translations'] = existing.get('translations', {})
                existing_content[content['slug']] = content

            all_content = list(existing_content.values())
            all_content = self.add_language_tags(all_content)

            all_content = self.pre_filters(all_content)

            all_content = self.merge_translations(all_content)
            all_content = self.fix_relationships(all_content)

            all_content = self.post_filters(all_content)

            self.download_assets()
            self.db_save(all_content)


    def pre_filters(self, all_content):
        '''
        Project-specific filtering 
        '''
        log('filters: preprocessing unmerged docs')

        # TODO: There are better ways to make a relative path
        asset_path_rel = self.config['asset-path'].lstrip('/')

        rwe_file_pattern = 'rwe_{md5}_{slug:.32}.jpg'
        rwe_is_generated = re.compile('rwe_[a-f0-9]{32}_').match

        # There are about 12 more dashes in unicode, but we'll support these
        # five for key-whatever modules and call it a day. This regex handles
        # incorrect spacing around the hyphens, Arabic hyphens and more!
        key_pattern = r'(?P<module>.+?)(?:{}[][)(]*[-—–―ـ]\s+|\s+[-—–―ـ]\s+)(?P<description>.+)'
        key_finder = re.compile(key_pattern.format(ARABIC_BOUNDARY_REGEX)).findall

        language_default = self.config['language-default']

        module_types = [t['one'] for t in self.config['types-modules']]
        module_types_plural = [t['many'] for t in self.config['types-modules']]

        for content in all_content:
            if NEW in content:
                # Add a module-type
                if content['type'] in module_types:
                    content['module-type'] = 'full'
                    if re.search('SNAPSHOT', content['document_title']):
                        content['module-type'] = 'snapshot'
                    elif re.search('GALLERY', content['document_title']):
                        content['module-type'] = 'gallery'

                # Clean up learn-more section
                content['learn-more'] = [L for L in content.get('learn-more', [])
                    if L.get('title') and L.get('link') and L.get('title') != 'abc' and L.get('link') != 'url']
                if not content['learn-more']:
                    del content['learn-more']

                # Clean up real-world-examples section
                content['real-world-examples'] = [e for e in content.get('real-world-examples', [])
                                                  if all(map(e.get, ['title','link','description']))]
                for e in content['real-world-examples']:
                    if 'image' not in e or rwe_is_generated(e['image']):
                        filename = rwe_file_pattern.format(md5=md5(e['link'].encode()).hexdigest(), slug=slugify(e['title']))
                        output_path = os.path.join(asset_path_rel, filename)
                        e['image'] = filename
                        log('siteprev: generating for "{}"'.format(e['title']))
                        venv_run('sitepreview', e['link'], output_path)
                if not content['real-world-examples']:
                    del content['real-world-examples']

                # Clean up some snapshots with example write ups
                full_write_up = content.get('full-write-up')
                if full_write_up and re.search(r'In a page \(500 words\) or less', full_write_up):
                    del content['full-write-up']

                # Clean up some modules with example tags
                tags = content.get('tags')
                if tags and len(tags) == 3 and all(t.lower() in ['corruption', 'mining', 'gender & sexuality'] for t in tags):
                    del content['tags']

                # Simplify the key-stuff
                # TODO: Slugify these 
                content['key-modules'] = {}
                for module_type in module_types_plural:
                    key_name = 'key-' + module_type
                    if key_name in content:
                        content['key-modules'][key_name] = [result[0] for result in (key_finder(k) for k in content[key_name]) if result]
                if not content['key-modules']:
                    del content['key-modules']

        return all_content


    def post_filters(self, all_content):
        '''
        As a final step, iterate through all modules, patch up module links
        and add bylines for simplicity
        '''
        log('filters: postprocessing merged docs')

        language_all = self.config['language-all']
        language_default = self.config['language-default']

        # Produce byline field for each language
        language_joiners = {
            'en': [', ', ' and '],
            'es': [', ', ' y '],
            'ar': [' \u0648 ', ' \u0648 '],
            'pt': [',', ' e '],
        }
        content_by_slug = {c['slug']: c for c in all_content}
        for content in all_content:
            author_content = [content_by_slug[a] for a in content.get('authors', [])]
            if author_content:
                author_by_lang = {lang: [a['translations'].get(lang, {}).get('title', a['title']) for a in author_content]
                                  for lang in language_all}
                for lang, authors in author_by_lang.items():
                    join_mid, join_last = language_joiners[lang]
                    if len(authors) == 1:
                        byline = authors[0]
                    elif len(authors) == 2:
                        byline = join_last.join(authors)
                    else:
                        byline = join_mid.join(authors[:-1]) + join_last + authors[-1]

                    if lang == language_default:
                        content['byline'] = byline
                    elif lang in content['translations']:
                        content['translations'][lang]['byline'] = byline

        # This regex isn't pefect, but should work for 99% of our cases. The
        # problem relates to detecting nested parens without a proper parser.
        # This solution just swallows any ending with an extra close paren.
        # Since the link text is fuzzy matched anyway, we don't need to reliably
        # capture the full link text. However, if any module names end up with
        # parens in their middles "Like (such as) this", this method will fail.
        xref_matcher = re.compile(r'(?<!!)\[([^\]]*)\]\(((?!http)[^)]+)\)(?:\s*\))?').search
        xref_pat = self.config['xref-link-pattern']
        markdown_fields = self.config['markdown']

        # Ideally these strings would live in some kind of configurable
        # place, but time constraints make that a low-priority
        xref_fmt_strings = {
            'en': '(see: [%s: %s]({}))',
            'es': '(ver: [%s: %s]({}))',
            'ar': '(انظر [%s: %s]({}))',
            'pt': '(veja: [%s: %s]({}))',
        }
        types = {
            'en': {'methodology': 'methodology', 'principle': 'principle',
                'story': 'story', 'tactic': 'tactic', 'theory': 'theory'},
            'es': {'methodology': 'metodología', 'principle': 'principio',
                'story': 'historia', 'tactic': 'táctica', 'theory': 'teoría'},
            'ar': {'methodology': 'منهجية', 'principle': 'مبدأ',
                'story': 'قصة', 'tactic': 'تكتيك', 'theory': 'نظرية'},
            'pt': {'methodology': 'metodologia', 'principle': 'princípio',
                'story': 'história', 'tactic': 'táctica', 'theory': 'teoria'},
        }

        def patch_links(text):
            m, chunks = xref_matcher(text), []
            while m:
                link_text, module_name, _, end = *m.groups(), *m.span()
                # TODO: ensure this gets the right language, even on a fresh load
                content = self.find_content(module_name, all_content, thresh=90)
                if content:
                    if link_text:
                        replacement = '[%s]({})'.format(xref_pat) % (link_text, content['slug'])
                    else:
                        if language == language_default:
                            link_text = nest_parens(content['title'], 1)
                        else:
                            try:
                                link_text = nest_parens(content['translations'][language]['title'], 1)
                            # Linking to a non-existent translation? Yikes. Insert the default language name.
                            except KeyError:
                                link_text = nest_parens(content['title'], 1)
                        type_name = types[language][content['type']].upper()
                        replacement = xref_fmt_strings[language].format(xref_pat) % (type_name, link_text, content['slug'])
                    chunks.append(re.sub(re.escape(m.group()), replacement, text[:end]))
                else:
                    #TODO: do something clever when a x-refed module is missing?
                    chunks.append(text[:end])
                text = text[end:]
                m = xref_matcher(text)
            return ''.join(chunks) + text

        # Recursive visitor reaches all deeply nested strings
        visit_all = lambda x: {
            list:   lambda L: [visit_all(i) for i in L],
            tuple:  lambda t: [visit_all(i) for i in t],
            dict:   lambda d: {k: visit_all(v) for k,v in d.items()},
            str:    lambda s: patch_links(s),
            int:    lambda i: i,
        }[dict if isinstance(x, dict) else type(x)](x)


        # Create a mapping of titles to slugs
        slugs_by_title = {}
        for content in all_content:
            slugs_by_title[content['title']] = content['slug']
            for c in content['translations'].values():
                if 'title' in c:
                    slugs_by_title[c['title']] = content['slug']
        titles = slugs_by_title.keys()

        # This final pass through all nested content patches up xrefs and key-modules
        for content in all_content:
            language = content['lang']
            # Add slugs to key-modules
            if 'key-modules' in content:
                for key_group in content['key-modules'].values():
                    for i, k in enumerate(key_group):
                        key_group[i] = list(k[:2]) + [slugs_by_title.get(self.find_fuzzy(k[0], titles, thresh=90), '')]
            # Process xref links in markdown fields
            if NEW in content and language in language_all:
                for field in self.config['markdown']:
                    if content.get(field):
                        content[field] = visit_all(content.get(field))
            # This should be the last time the NEW marker is needed
            content.pop(NEW, None)

            for language, c in content['translations'].items():
                # Add slugs to nested key-modules
                if 'key-modules' in c:
                    for key_group in c['key-modules'].values():
                        for i, k in enumerate(key_group):
                            key_group[i] = list(k[:2]) + [slugs_by_title.get(self.find_fuzzy(k[0], titles, thresh=90), '')]
                # Process xref links in markdown fields
                if NEW in c and language in language_all:
                    for field in self.config['markdown']:
                        if c.get(field):
                            c[field] = visit_all(c.get(field))
                # This should be the last time the NEW marker is needed
                # NOTE these remain in the translated pieces unless removed here
                c.pop(NEW, None)

        return all_content


    def find_fuzzy(self, title, title_list, thresh=50):
        '''
        General-purpose fuzzy matcher
        '''
        match = fuzzy(title, title_list)
        if match and match[1] >= thresh:
            return match[0]


    def find_content(self, item_name, item_list, thresh=50, fuzzy_match_cache={}):
        '''
        Use fuzzy matching to find a content item from a list
        This should always return a dict

        XXX: If item_list has changed since the last time it was cached, this function
             can return an item which is no longer in item_list. A source of strange bugs.
        '''
        if not isinstance(item_name, str):
            return {}
        cached = fuzzy_match_cache.setdefault(id(item_list), {}).get(item_name)
        if cached:
            return cached
        match = fuzzy(item_name, item_list, processor=lambda m: m.get('title', ''))
        if match and match[1] >= thresh:
            fuzzy_match_cache[id(item_list)][item_name] = match[0]
            return match[0]
        return {}


    def db_get_or_create(self):
        '''
        Get the database, creating if necessary
        '''
        self.db = self.couch[DB_NAME] if DB_NAME in self.couch else self.couch.create(DB_NAME)
        return self.db


    def db_save(self, doc_or_docs):
        '''
        Write one or many dicts (docs) to couchdb
        '''
        if not doc_or_docs: return
        # Handle one or many docs
        docs = [doc_or_docs] if isinstance(doc_or_docs, dict) else doc_or_docs
        # Remove couch-disallowed keys and add _id where needed
        docs = [{k:v for k,v in d.items() if k in ('_id', '_rev') or not k.startswith('_')} for d in docs]
        [d.update(_id='{type}:{slug}'.format(**d)) for d in docs if '_id' not in d]
        # Simple conflict resolution (WARNING: this won't work with replication!)
        for success,id,rev_or_exc in self.db.update(docs):
            if isinstance(rev_or_exc, couchdb.http.ResourceConflict):
                retry = [d for d in docs if d['_id'] == id][0]
                retry.update(_rev=self.db[id]['_rev'])
                self.db.save(retry)


    def configure(self):
        '''
        Fetch, parse, set defaults, and store the config
        '''
        # Cap couchdb revision limit since documents are so frequently updated
        requests.put(urljoin(DB_SERVER, DB_NAME+'/_revs_limit'), data='50').status_code

        # Load configuration document and set defaults
        document = self.root.file(DRIVE_CONFIG_FILE_NAME)
        if not document:
            die("Can't find a config file!")
        self.config = c = parse_archieml(document.text)

        # Language settings
        c.setdefault('language-default', 'en')
        c.setdefault('language-all', ['en'])
        c.setdefault('language-add', '')
        c.setdefault('language-remove', '')
        c.setdefault('language-detection-weighted-keys', [])
        # How we distinguish published content
        c.setdefault('published-filename-regex', r'(?i)\bdone$')
        # Renaming synonymous keys, including those with language-suffixes
        c.setdefault('synonyms', {})
        # Manage single keys which contain lists
        c.setdefault('plural-separator-regex', r'(?:\s*,|\s+and|\s+&)\s+')
        c.setdefault('plural-keys', {})
        # Fields which should be parsed with markdown parser
        c.setdefault('markdown', [])
        c.setdefault('xref-link-pattern', '/tool/%s')
        # Fields which should be indexed by client search engines
        c.setdefault('search', [])
        # Download top-level assets from this top-level folder to this (relative) local path
        c.setdefault('asset-sources', ['ASSETS'])
        c.setdefault('asset-path', '/assets/content')
        c.setdefault('asset-manipulation', {})
        # Content type information
        c['types'] = []
        for key,value in c.items():
            if key.startswith('types-'):
                c['types'] += value
        c['plural-name-for-type'] = {T['one']: T['many'] for T in c['types']}
        c['singular-name-for-type'] = {T['many']: T['one'] for T in c['types']}

        # Relationships between content items are specified as pairs of one-way fields and groups of pairs of two-way fields
        c['relationships'] = {'forward': {}, 'backward': []}
        for key,value in c.items():
            if key.startswith('one-way') or key.startswith('two-way'):
                c['relationships']['forward'].update(value)
            if key.startswith('two-way'):
                c['relationships']['backward'].append(value)

        # Save the config before creating lots of temporary language-related data within it
        c.update(type='config', slug='api')
        self.db_save(c)
        log('load: configuration options from drive document "{}"'.format(DRIVE_CONFIG_FILE_NAME))

        # Key transformations have to take into account language suffixes, so this adds suffixed copies
        # of synonyms and plural-keys
        add_language_suffixes = lambda D: [D.update(each) for each in
            [{k+'-'+lang: [i+'-'+lang for i in v] if isinstance(v, list) else
                          v+'-'+lang for k,v in D.items()} for lang in self.config['language-all']] ]
        add_language_suffixes(c['synonyms'])
        add_language_suffixes(c['plural-keys'])


    def watch(self):
        '''
        Request push notifications for entire drive be sent to the API_NOTIFICATION_PATH
        '''
        url = urljoin(API_SERVER, API_NOTIFICATION_PATH)
        now = datetime.utcnow()
        expiration = int((60*60*24 + now.timestamp()) * 1000) # UTC + 24h in ms
        self.drive.execute(self.drive.service.changes().watch(body={
            'id': '{}-{}'.format(DRIVE_CLIENT_NAME, expiration),
            'type': 'web_hook',
            'address': url,
            'token': API_NOTIFICATION_TOKEN,
            'expiration': expiration,
        }))
        log("watch: for push notifications at {}".format(url))
    
    
    def unwatch(self):
        '''
        Request all push notification channels in db be cancelled
        '''
        if 'config:notification-channels' in self.db:
            for channel,resource in self.db['config:notification-channels'].items():
                if channel not in ('_id', '_rev'):
                    try:
                        self.drive.execute(self.drive.service.channels().stop(body={
                            'id': channel, 
                            'resourceId': resource,
                        }))
                        warn("stop: Channel-Id: {} Resource-Id: {}".format(channel, resource))
                    except: pass
            del self.db['config:notification-channels']


    def get_documents(self):
        '''
        Get documents by file ids, change ids or all documents
        '''
        documents = []
        # Get only the specifically requested documents by id or change id
        if self.options.ids or self.options.changes:
            documents.extend(d for d in (self.drive.get(id) for id in self.options.ids) if d)
            documents.extend(d for d in (self.drive.get_change(id) for id in self.options.changes) if d)
        # Get all documents
        else:
            # Recursive folder getter requires python3.3+ for "yield from"
            def get_folders(root):
                for folder in root.folders:
                    yield folder
                    yield from get_folders(folder)
            for folder in get_folders(self.root):
                documents.extend(folder.documents)
                log('find: content in drive folder "{}"'.format(folder.title))
        return documents


    def extract_and_transform(self, document):
        '''
        Process a document and return a content item.
        '''
        content = parse_archieml(document.text)
        content[NEW] = True

        # Rename synonymous keys (this should happen before all other transformations)
        for old_key,new_key in self.config['synonyms'].items():
            old_value = content.get(old_key)
            if old_value is not None:
                content[new_key] = old_value
                del content[old_key]

        # Determine the type
        type = next((T for T in self.config['types'] if T['one'] in content), {}).get('one', '')
        content['type'] = type
        content['title'] = title = content.get(type)
        if not isinstance(title, str): 
            warn("skip: {} bad type information".format(document.id))
            return

        # Add a few useful bits
        content['slug'] = slugify(content.get('title', ''), allow='')
        content['document_id'] = document.id
        content['document_link'] = document.alternateLink
        content['document_title'] = document.title
        try:
            dt = parser.parse(content['date'])
        except: # Easier to ask forgiveness...
            dt = parser.parse(document.modifiedDate)
        content['timestamp'] = int(1000 * dt.timestamp())

        # Convert singular keys to plural keys and split them up as lists
        for plural_key,singular_key in self.config['plural-keys'].items():
            single, plural = content.get(singular_key), content.get(plural_key)
            if single:
                content[plural_key] = [single]
                if plural_key != singular_key:
                    del content[singular_key]
            if plural and not isinstance(plural, list):
                multiline = re.split(r'\s*\n\s*\n\s*', plural)
                content[plural_key] = (multiline if len(multiline) > 1 else 
                                       re.split(self.config['plural-separator-regex'], plural))

        log("extract: {} ({}: {})".format(document.id, type, content['title']))
        return content


    def add_language_tags(self, all_content):
        '''
        Detect the language of each content item and add a language tag

        A document can specify its language with a lang: value. Otherwise it will be
        determined from a corpus of values whose keys specify no language suffix, 
        favoring more heavily those keys specified with the configuration item
        called language-detection-weighted-keys.

        Language detection is difficult to perform correctly in an automatic way. 
        Some documents are inherently multilingual and others are inherently 
        independent of language. This method attempts to do an intelligent thing for
        most cases, but it could be smarter. If you find yourself questioning the 
        process herein, by all means improve it!
        '''

        # Get the set of possible suffixes to weed out text irrelevant for detection
        langdetect.detector_factory.init_factory()
        language_suffixes = {'-'+lang for lang in langdetect.detector_factory._factory.langlist}

        language_default = self.config['language-default']
        weighted_keys = {k for k in self.config['language-detection-weighted-keys']}
        omitted_keys = {'_id', '_rev', 'type', 'slug', 'timestamp', 'translations', 
                        'document_id', 'document_link', 'document_title'}

        # Matches http/s, emails and 3-character-suffixed filenames
        an_obvious_computer_thing = re.compile(r'(http|[^\s]+(\.[a-z]{3}|@[^\s]+)$)').match
        # This recursive function concatenates text from nested structures
        r_concat = lambda x: {
            list:   lambda L: '\n'.join(map(r_concat, L)),
            dict:   lambda d: '\n'.join(map(r_concat, d.values())),
            str:    lambda s: '' if an_obvious_computer_thing(s) else s
        }.get(dict if isinstance(x, dict) else type(x), str)(x)

        for content in all_content:
            if 'lang' not in content:
                text_items = {k: r_concat(v) for k,v in content.items()
                              if k[-3:] not in language_suffixes and k not in omitted_keys}
                
                # Create two identical language guesses (lists) as fallback for the case when detection
                # fails on either or both. Both are low confidence guesses of the default language, with
                # the guess being extremely low confidence when a translation-specific key is present.
                lang, lang_weighted = [[langdetect.language.Language(language_default,
                                        0.5 if 'default-language-content' in content else 0.7)]] * 2

                # A high confidence non-weighted guess will be squashed a little, or "normalized"
                corpus = '\n'.join(text_items.values())
                try:
                    lang = langdetect.detect_langs(corpus)[:1]
                    lang[0].prob = 1 / (1 + math.exp(-lang[0].prob))
                    # ^ Rather than using this clumsy sigmoid function to squash high values, it might
                    #   make more sense to compare the deviation between all guesses and the top two,
                    #   then make a decision about the weighted guess based on this difference. Multi-
                    #   lingal docs would have low deviation between the top two guesses when compared
                    #   with deviation overall, and a weighted guess would break the tie.
                except langdetect.lang_detect_exception.LangDetectException: ...

                # A "weighted guess" (based on >20 chars of weighted keys) is left as-is
                corpus_weighted = '\n'.join(v for k,v in text_items.items() if k in weighted_keys)
                if len(corpus_weighted) > 20:
                    try: 
                        lang_weighted = langdetect.detect_langs(corpus_weighted)
                    except langdetect.lang_detect_exception.LangDetectException: ...

                # And the best guess wins.
                language = max(lang + lang_weighted).lang
                content['lang'] = language

        return all_content


    def merge_translations(self, all_content):
        '''
        Assuming all necessary documents have already been fetched, merge translations
        into the content object for the default language. They will be placed into a 
        'translations' dictionary under two-letter language code keys.
        '''
        language_all = self.config['language-all']
        language_default = self.config['language-default']
        language_other = set(language_all) - set(language_default)
        #language_add = self.config['language-add']
        #language_remove = self.config['language-remove']
        content_primary = []
        content_translated = []

        # Sort content by language
        for content in all_content:
            if content['lang'] == language_default:
                content_primary.append(content)
            else:
                content_translated.append(content)

        # Add translated content to a translations dict in each default language piece
        for translation in content_translated:
            default = self.find_content(translation.get('default-language-content', ''), content_primary, thresh=90)
            if not default:
                warn("skip: {} can't find default language version {}".format(translation.get('title'), translation.get('default-language-content')))
                continue
            translations = default.setdefault('translations', {})
            translations[translation['lang']] = translation
            log("merge: {} ({}) => {}".format(translation['title'], translation['lang'], default['title']))

        # Recursive in-place dictionary merging function to be used on COPIES of destination dicts
        def merge_dicts(dest, src):
            for k, v in src.items():
                if isinstance(dest.get(k), dict) and isinstance(v, dict):
                    merge_dicts(dest[k], v)
                else:
                    dest[k] = v

        # Look through primary language documents and integrate keys with language code suffixes (-es, -fr)
        for content in content_primary:
            content.setdefault('translations', {})
            # Merge the default language first, simply replacing objects without merging subkeys
            default_language_keys = [k for k in content if isinstance(k, str) and k.endswith('-' + language_default)]
            content.update({k[:-3]: content[k] for k in default_language_keys if content[k]})
            [content.pop(k) for k in default_language_keys]
            # Merge the remaining languages into the default language
            for lang in language_other:
                # Get inline translations to be merged and remove them from the content object
                language_keys = [k for k in content if isinstance(k, str) and k.endswith('-' + lang)]
                language_new = {k[:-3]: content[k] for k in language_keys if content[k]}
                [content.pop(k) for k in language_keys]
                if language_new:
                    # Get any existing translations and update the simple keys
                    language_dict = content['translations'].setdefault(lang, {})
                    language_dict.update(language_new)
                    # Copy dicts from the original content, merge translations into them, then update the existing translations
                    default_language_dicts_to_merge_into = {k: deepcopy(content[k]) for k,v in language_new.items()
                                                            if isinstance(content.get(k), dict)}
                    [merge_dicts(v, language_new[k]) for k,v in default_language_dicts_to_merge_into.items()]
                    language_dict.update(default_language_dicts_to_merge_into)

        # TODO: language-add / language-remove (currently performed by API server)

        # All content is now in this merged list
        return content_primary


    def fix_relationships(self, all_content):
        '''
        Replace relationships based on document titles with fuzzy-matched slugs
        '''
        typed_content = {}
        slugged_content = {}
        for content in all_content:
            typed_content.setdefault(content['type'], []).append(content)
            slugged_content[content['slug']] = content

        # Forward relationships are specified with a mapping of fields to types. Simply replace titles with 
        # fuzzy-matched slugs, eliminate invalid entries, and remove empty fields.
        forward = self.config['relationships']['forward']
        for content in all_content:
            for field,doc_type in forward.items():
                related_titles = content.get(field)
                if related_titles is not None:
                    if doc_type == 'any':
                        possibly_related_docs = all_content
                    else:
                        possibly_related_docs = typed_content.get(doc_type, [])
                    if isinstance(related_titles, list):
                        # Ignore leading hyphens when sorting (Is this sort redundant? Prove it before removing!)
                        content[field] = sorted((c['slug'] for c in (self.find_content(t, possibly_related_docs, 90) for t in related_titles) if c),
                            key=lambda s: s.lstrip('-'))
                    elif isinstance(related_titles, str):
                        content[field] = self.find_content(related_titles, possibly_related_docs, 90).get('slug')
                    if not content[field]:
                        del content[field]

        # Backward relationships ensure that groups of interrelated content are linked in both directions,
        # even when related content is only specified going one way. Each group of backward relationships 
        # specifies two things:
        #   1. The fields containing forward relationships so that forward related docs can be found
        #   2. A mapping of types to the fields which, in those related docs, need to be related back
        backward_groups = [(g, {v: k for k, v in g.items()}) for g in self.config['relationships']['backward']]
        for content in all_content:
            slug = content['slug']
            # Each of these ({field:type}, {type:field}) pairs represents a group of interrelated content
            for fields_to_types, types_to_fields in backward_groups:
                # Field relating back to this type
                field_back = types_to_fields.get(content['type'])
                if field_back:
                    # Iterate through everything ~this~ doc relates to and get those docs
                    related_docs = []
                    for field in fields_to_types:
                        related_slugs = content.get(field)
                        if related_slugs:
                            if isinstance(related_slugs, str):
                                related_slugs = [related_slugs]
                            if isinstance(related_slugs, list):
                                related_docs.extend([*filter(None, (slugged_content.get(s) for s in related_slugs))])

                    # Now add the backward relationship to every doc
                    for doc in related_docs:
                        backward_field = doc.get(field_back)
                        if isinstance(backward_field, str):
                            doc[field_back] = slug
                        elif backward_field is None:
                            doc[field_back] = [slug]
                        elif isinstance(backward_field, list):
                            # Ignore leading hyphens when sorting
                            doc[field_back] = sorted({slug} | set(backward_field), key=lambda s: s.lstrip('-'))
        return all_content


    def download_assets(self, force_conversion=False):
        '''
        Download all top-level assets from top-level folders specified in config['asset-sources']
        '''
        # In case somebody uses this feature to write to a production server :(
        clean_path = lambda p: os.path.normpath(p.replace('\0','').replace('..','').strip('/'))

        destination = clean_path(self.config['asset-path'])
        with script_subdirectory(destination):
            for source in self.config['asset-sources']:
                folder = self.root.folder(source)
                for file in folder.files:
                    convert = force_conversion
                    if file.save_as(file.title):
                        log('download: asset "{}"'.format(file.title))
                        convert = True
                    if convert and file.attributes['mimeType'] in ('image/png', 'image/jpeg'):
                        log('convert: asset "{}"'.format(file.title))
                        for prefix,args in self.config['asset-manipulation'].items():
                            Popen(['convert', *shlex.split(args),
                                   file.title, '{}-{}'.format(prefix, file.title)])


if __name__ == '__main__':
    with only_one_process(DB_NAME):
        ContentLoader()

