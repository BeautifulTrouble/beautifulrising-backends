#!/usr/bin/env python3
#encoding: utf-8

# Auto-install and activate a virtualenv if possible
import autovenv
autovenv.run()

import argparse
import os
import re
import sys
from copy import deepcopy
from datetime import datetime
from itertools import zip_longest
from pprint import pprint
from string import capwords
from subprocess import Popen
from urllib.parse import urljoin

import couchdb
import driveclient
import jinja2
import langdetect
import requests
from fuzzywuzzy.process import extractOne as fuzzy

from utils import *
from config import *


class ContentLoader(object):
    def __init__(self):
        # Parse command line arguments
        arg_parser = argparse.ArgumentParser(description="")
        arg_parser_group = arg_parser.add_mutually_exclusive_group()
        arg_parser_group.add_argument('--id', type=str, metavar='DOCUMENT_ID', action='append', 
            default=[], dest='ids', help="Fetch a single document by its globally unique id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--change-id', type=str, metavar='CHANGE_ID', action='append', 
            default=[], dest='changes', help="Fetch a single document by its ephemeral change id, "
            "then update associated metadata and assets. Specify multiple times to get multiple "
            "documents at once.")
        arg_parser_group.add_argument('--assets', action='store_true', help="Download and convert "
            "all assets. Use after changing asset settings in the config.")
        arg_parser_group.add_argument('--watch-docs', action='store_true',
            help="Initiate a request to watch drive for changes. It will expire in one day.")
        arg_parser_group.add_argument('--stop-watching', action='store_true', 
            help="If the db has any record of a watch request, request that it be cancelled.")
        arg_parser_group.add_argument('--delete-db', action='store_true',
            help='Delete any existing database named "{}".'.format(DB_NAME))
        self.options,_ = arg_parser.parse_known_args()

        # Connect to couchdb
        self.couch = couchdb.Server(DB_SERVER)
        self.db = self.couch[DB_NAME] if DB_NAME in self.couch else self.couch.create(DB_NAME)

        # Connect to Google Drive and get the root folder
        self.drive = driveclient.DriveClient(DRIVE_CLIENT_NAME,
            scopes='https://www.googleapis.com/auth/drive',
            service_account_json_filename=DRIVE_SERVICE_ACCOUNT_JSON_FILENAME)
        self.root = self.drive.folder(DRIVE_ROOT_FOLDER_NAME) 
        if not self.root:
            die("Can't find the root folder!")

        # Load DRIVE_CONFIG_FILE_NAME from DRIVE_ROOT_FOLDER_NAME and store as "config:api" & self.config
        self.configure()

        # Download assets
        if self.options.assets:
            self.download_assets(force_conversion=True)

        # Watch for changes
        elif self.options.watch_docs:
            self.unwatch()
            self.watch()

        # Stop watching for changes
        elif self.options.stop_watching:
            self.unwatch()

        # Delete the database
        elif self.options.delete_db:
            confirm = input('Delete the database "{}" [y/N]? '.format(DB_NAME))
            if confirm.lower() == 'y':
                self.unwatch()
                del self.couch[DB_NAME]

        # Load content
        else:
            #TODO: in production always unwatch/watch
            #TODO: handle document renaming/deletion

            # Fetch all the existing content from the db
            existing_content = {d.doc['slug']: d.doc for d in self.db.view('_all_docs', include_docs=True) 
                                if 'document_id' in d.doc}

            # Identify published documents by their filenames and fetch new content
            published = re.compile(self.config['published-filename-regex']).search
            published_documents = (d for d in self.get_documents() if published(d.title))
            new_content = filter(None, map(self.extract_and_transform, published_documents))

            # TODO: detect duplicate names

            # Merge new content with existing, preserving revision number and translations
            for content in new_content:
                existing = existing_content.get(content['slug'])
                #TODO: this is a bug and should be fixed (duplicate titles -> duplicate slugs -> duplicate ids -> incorrect reasoning about what is or isn't an existing piece)
                if existing and '_rev' in existing: 
                    content['_rev'] = existing['_rev']
                    content['translations'] = existing.get('translations', {})
                existing_content[content['slug']] = content

            all_content = list(existing_content.values())
            all_content = self.add_language_tags(all_content)

            all_content = self.custom_filters(all_content)

            all_content = self.merge_translations(all_content)
            all_content = self.fix_relationships(all_content)
            self.download_assets()
            self.db_save(all_content)


    def custom_filters(self, all_content):
        '''
        Project-specific filtering 
        '''
        matcher = re.compile(r'(?<!!)\[([^\]]*)\]\(((?!http)[^)]+)\)').search
        xrefpat = self.config['xref-link-pattern']
        language_default = self.config['language-default']

        def patch_links(text):
            m, chunks = matcher(text), []
            while m:
                link_text, module_name, _, end = *m.groups(), *m.span()
                content = self.find_content(module_name, all_content, thresh=90)
                if content:
                    if link_text:
                        replacement = '[%s]({})'.format(xrefpat) % (link_text, content['slug'])
                    else:
                        replacement = '(see: [%s: %s]({}))'.format(xrefpat) % (content['type'].upper(), content['title'], content['slug'])
                    chunks.append(re.sub(re.escape(m.group()), replacement, text[:end]))
                else:
                    #TODO: do something clever for missing modules?
                    chunks.append(text[:end])
                text = text[end:]
                m = matcher(text)
            return ''.join(chunks) + text

        # Recursive visitor reaches all deeply nested strings
        visit_all = lambda x: {
            list:   lambda L: [visit_all(i) for i in L],
            dict:   lambda d: {k: visit_all(v) for k,v in d.items()},
            str:    lambda s: patch_links(s),
            int:    lambda i: i,
        }[dict if isinstance(x, dict) else type(x)](x)

        module_types = [t['one'] for t in self.config['types-modules']]
        for content in all_content:
            if '_new_content' in content:
                # Add a module-type
                if content['type'] in module_types:
                    content['module-type'] = 'full'
                    if re.search('SNAPSHOT', content['document_title']):
                        content['module-type'] = 'snapshot'
                    elif re.search('GALLERY', content['document_title']):
                        content['module-type'] = 'gallery'
                # Clean up learn-more section
                learn = content.get('learn-more')
                if learn:
                    content['learn-more'] = [L for L in learn 
                        if L.get('title') and L.get('link') and L.get('title') != 'abc' and L.get('link') != 'url']
                # Clean up real-world-examples section
                examples = content.get('real-world-examples')
                if examples:
                    content['real-world-examples'] = [e for e in examples if all(map(e.get, ['title','link','description']))]
                # Clean up some snapshots with example write ups
                full_write_up = content.get('full-write-up')
                if full_write_up and re.search(r'In a page \(500 words\) or less', full_write_up):
                    del content['full-write-up']
                # Clean up some modules with example tags
                tags = content.get('tags')
                if tags and all(t.lower() in ['corruption', 'mining', 'gender & sexuality'] for t in tags):
                    del content['tags']
                # Patch up links between modules
                for field in self.config['markdown']:
                    if content.get(field):
                        content[field] = visit_all(content.get(field))
                # Produce a byline key
                authors = content.get('authors')
                if authors and content['lang'] == language_default:
                    authors = sorted(authors)
                    if len(authors) == 1:
                        content['byline'] = authors[0]
                    elif len(authors) == 2:
                        content['byline'] = ' and '.join(authors)
                        content['byline-es'] = ' y '.join(authors)
                        content['byline-ar'] = ' \u0648 '.join(authors)
                    else:
                        content['byline'] = ', '.join(authors[:-1]) + ', and ' + authors[-1]
                        content['byline-es'] = ', '.join(authors[:-1]) + ', y ' + authors[-1]
                        content['byline-ar'] = ' \u0648 '.join(authors)
                del content['_new_content']
        return all_content


    def find_content(self, item_or_name, item_list, thresh=50, fuzzy_match_cache={}):
        '''
        Use fuzzy matching to find a content item from a list
        '''
        item_name = item_or_name
        if not isinstance(item_or_name, str):
            item_name = item_or_name['title']
        cached = fuzzy_match_cache.get(item_name)
        if cached:
            return cached
        match = fuzzy(item_name, item_list, processor=lambda m: m['title'])
        if match and match[1] >= thresh:
            fuzzy_match_cache[item_name] = match[0]
            return match[0]
        return {}


    def db_save(self, doc_or_docs):
        '''
        Write one or many dicts (docs) to couchdb
        '''
        if not doc_or_docs: return
        # Handle one or many docs
        docs = [doc_or_docs] if isinstance(doc_or_docs, dict) else doc_or_docs
        # Remove couch-disallowed keys and add _id where needed
        docs = [{k:v for k,v in d.items() if k in ('_id', '_rev') or not k.startswith('_')} for d in docs]
        [d.update(_id='{type}:{slug}'.format(**d)) for d in docs if '_id' not in d]
        # Simple conflict resolution (WARNING: this won't work with replication!)
        for success,id,rev_or_exc in self.db.update(docs):
            if isinstance(rev_or_exc, couchdb.http.ResourceConflict):
                retry = [d for d in docs if d['_id'] == id][0]
                retry.update(_rev=self.db[id]['_rev'])
                self.db.save(retry)


    def configure(self):
        '''
        Fetch, parse, set defaults, and store the config
        '''
        # Cap couchdb revision limit since documents are so frequently updated
        requests.put(urljoin(DB_SERVER, DB_NAME+'/_revs_limit'), data='50').status_code

        # Load configuration document and set defaults
        document = self.root.file(DRIVE_CONFIG_FILE_NAME)
        if not document:
            die("Can't find a config file!")
        self.config = c = parse_archieml(document.text)

        # Language settings
        c.setdefault('language-default', 'en')
        c.setdefault('language-all', ['en'])
        c.setdefault('language-add', '')
        c.setdefault('language-remove', '')
        # How we distinguish published content
        c.setdefault('published-filename-regex', r'(?i)\bdone$')
        # Renaming synonymous keys, including those with language-suffixes
        c.setdefault('synonyms', {})
        # Manage single keys which contain lists
        c.setdefault('plural-separator-regex', r'(?:\s*,|\s+and|\s+&)\s+')
        c.setdefault('plural-keys', {})
        # Relationships between content items
        c.setdefault('relationships', {})
        # Fields which should be parsed with markdown parser
        c.setdefault('markdown', [])
        c.setdefault('xref-link-pattern', '/tool/%s')
        # Fields which should be indexed by client search engines
        c.setdefault('search', [])
        # Download top-level assets from this top-level folder to this (relative) local path
        c.setdefault('asset-sources', ['ASSETS'])
        c.setdefault('asset-path', '/assets/content')
        c.setdefault('asset-image-sizes', {})
        c.setdefault('asset-image-quality', '85%')
        # Content type information
        c['types'] = []
        for key,value in c.items():
            if key.startswith('types-'):
                c['types'] += value

        # Save the config before creating lots of temporary language-related data within it
        c.update(type='config', slug='api')
        self.db_save(c)
        log('load: configuration options from drive document "{}"'.format(DRIVE_CONFIG_FILE_NAME))

        # Key transformations have to take into account language suffixes, so this adds suffixed copies
        # of synonyms and plural-keys
        add_language_suffixes = lambda D: [D.update(each) for each in
            [{k+'-'+lang: [i+'-'+lang for i in v] if isinstance(v, list) else
                          v+'-'+lang for k,v in D.items()} for lang in self.config['language-all']] ]
        add_language_suffixes(c['synonyms'])
        add_language_suffixes(c['plural-keys'])


    def watch(self):
        '''
        Request push notifications for entire drive be sent to the API_NOTIFICATION_URL
        '''
        now = datetime.utcnow()
        expiration = int((60*60*24 + now.timestamp()) * 1000) # UTC + 24h in ms
        self.drive.execute(self.drive.service.changes().watch(body={
            'id': '{}-{}'.format(DRIVE_CLIENT_NAME, expiration),
            'type': 'web_hook',
            'address': API_NOTIFICATION_URL,
            'token': API_NOTIFICATION_TOKEN,
            'expiration': expiration,
        }))
        log("watch: for push notifications at {}".format(API_NOTIFICATION_URL))
    
    
    def unwatch(self):
        '''
        Request all push notification channels in db be cancelled
        '''
        if 'config:notification-channels' in self.db:
            for channel,resource in self.db['config:notification-channels'].items():
                if channel not in ('_id', '_rev'):
                    try:
                        self.drive.execute(self.drive.service.channels().stop(body={
                            'id': channel, 
                            'resourceId': resource,
                        }))
                        warn("stop: Channel-Id: {} Resource-Id: {}".format(channel, resource))
                    except: pass
            del self.db['config:notification-channels']


    def get_documents(self):
        '''
        Get documents by file ids, change ids or all documents
        '''
        documents = []
        # Get only the specifically requested documents by id or change id
        if self.options.ids or self.options.changes:
            documents.extend(d for d in (self.drive.get(id) for id in self.options.ids) if d)
            documents.extend(d for d in (self.drive.get_change(id) for id in self.options.changes) if d)
        # Get all documents
        else:
            # Recursive folder getter requires python3.3+ for "yield from"
            def get_folders(root):
                for folder in root.folders:
                    yield folder
                    yield from get_folders(folder)
            for folder in get_folders(self.root):
                documents.extend(folder.documents)
                log('find: content in drive folder "{}"'.format(folder.title))
        return documents


    def extract_and_transform(self, document):
        '''
        Process a document and return a content item.
        '''
        content = parse_archieml(document.text)
        content['_new_content'] = True

        # Rename synonymous keys (this should happen before all other transformations)
        for old_key,new_key in self.config['synonyms'].items():
            old_value = content.get(old_key)
            if old_value is not None:
                content[new_key] = old_value
                del content[old_key]

        # Determine the type
        type = next((T for T in self.config['types'] if T['one'] in content), {}).get('one', '')
        content['type'] = type
        content['title'] = title = content.get(type)
        if not isinstance(title, str): 
            warn("skip: {} bad type information".format(document.id))
            return

        # Add a few useful bits
        content['slug'] = slugify(content.get('title', ''), allow=':')
        content['timestamp'] = int(1000 * datetime.strptime(document.modifiedDate, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp())
        content['document_id'] = document.id
        content['document_link'] = document.alternateLink
        content['document_title'] = document.title

        # Convert singular keys to plural keys and split them up as lists
        for plural_key,singular_key in self.config['plural-keys'].items():
            single, plural = content.get(singular_key), content.get(plural_key)
            if single:
                content[plural_key] = [single]
                if plural_key != singular_key:
                    del content[singular_key]
            if plural and not isinstance(plural, list):
                multiline = re.split(r'\s*\n\s*\n\s*', plural)
                content[plural_key] = (multiline if len(multiline) > 1 else 
                                       re.split(self.config['plural-separator-regex'], plural))

        log("extract: {} ({}: {})".format(document.id, type, content['title']))
        return content


    def add_language_tags(self, all_content):
        '''
        Detect the language of each content item and add a language tag
        '''
        language_suffixes = ['-'+lang for lang in self.config['language-all']]
        for content in all_content:
            if 'lang' not in content:
                corpus = '\n'.join(v for k,v in content.items() if isinstance(v, str)
                                   and (k[-3:] not in language_suffixes)
                                   and (not k.startswith('document_')))
                try:
                    lang = langdetect.detect(corpus)
                    assert lang in self.config['language-all']
                except:
                    lang = self.config['language-default']
                content['lang'] = lang
        return all_content


    def merge_translations(self, all_content):
        '''
        Assuming all necessary documents have already been fetched, merge translations
        into the content object for the default language. They will be placed into a 
        'translations' dictionary under two-letter language code keys.
        '''
        language_all = self.config['language-all']
        language_default = self.config['language-default']
        language_other = set(language_all) - set(language_default)
        #language_add = self.config['language-add']
        #language_remove = self.config['language-remove']
        content_primary = []
        content_translated = []

        # Sort content by language
        for content in all_content:
            if content['lang'] == language_default:
                content_primary.append(content)
            else:
                content_translated.append(content)

        # Add translated content to a translations dict in each default language piece
        for translation in content_translated:
            default = self.find_content(translation.get('default-language-content', ''), content_primary, thresh=90)
            if not default:
                warn("skip: {} can't find default language version {}".format(translation.get('title'), translation.get('default-language-content')))
                continue
            translations = default.setdefault('translations', {})
            translations[translation['lang']] = translation
            log("merge: {} ({}) => {}".format(translation['title'], translation['lang'], default['title']))

        # Recursive in-place dictionary merging function to be used on COPIES of destination dicts
        def merge_dicts(dest, src):
            for k, v in src.items():
                if isinstance(dest.get(k), dict) and isinstance(v, dict):
                    merge_dicts(dest[k], v)
                else:
                    dest[k] = v

        # Look through primary language documents and integrate keys with language code suffixes (-es, -fr)
        for content in content_primary:
            content.setdefault('translations', {})
            # Merge the default language first, simply replacing objects without merging subkeys
            default_language_keys = [k for k in content if isinstance(k, str) and k.endswith('-' + language_default)]
            content.update({k[:-3]: content[k] for k in default_language_keys if content[k]})
            [content.pop(k) for k in default_language_keys]
            # Merge the remaining languages into the default language
            for lang in language_other:
                # Get inline translations to be merged and remove them from the content object
                language_keys = [k for k in content if isinstance(k, str) and k.endswith('-' + lang)]
                language_new = {k[:-3]: content[k] for k in language_keys if content[k]}
                [content.pop(k) for k in language_keys]
                if language_new:
                    # Get any existing translations and update the simple keys
                    language_dict = content['translations'].setdefault(lang, {})
                    language_dict.update(language_new)
                    # Copy dicts from the original content, merge translations into them, then update the existing translations
                    default_language_dicts_to_merge_into = {k: deepcopy(content[k]) for k,v in language_new.items()
                                                            if isinstance(content.get(k), dict)}
                    [merge_dicts(v, language_new[k]) for k,v in default_language_dicts_to_merge_into.items()]
                    language_dict.update(default_language_dicts_to_merge_into)

        # TODO: language-add / language-remove (currently performed by API server)

        # All content is now in this merged list
        return content_primary


    def fix_relationships(self, all_content):
        '''
        Replace relationships based on titles with fuzzy-matched slugs
        '''
        # Create a mapping between keys and possible content items
        typed = {}
        for content in all_content:
            typed.setdefault(content['type'], []).append(content)

        relationships = self.config['relationships']
        keys_and_choices = {k: typed.get(v, []) for k,v in relationships.items()}
        types_and_keys = {v: k for k,v in relationships.items()}

        # Fuzzy match and slugify related content items
        for content in all_content:
            this_slug = {content['slug']}
            key_referring_to_this_type = types_and_keys.get(content['type'])
            for key,choices in keys_and_choices.items():
                if key in content:
                    found_content = {c['slug']: c for c in (self.find_content(t, choices, 90)
                                                            for t in content[key]) if c}
                    content[key] = sorted(found_content)
                    # If anything else relates to this object's type, make relationships bi-directional
                    if key_referring_to_this_type:
                        for other in found_content.values():
                            other_list = other.get(key_referring_to_this_type, [])
                            other[key_referring_to_this_type] = sorted(this_slug | set(other_list))
        return all_content


    def download_assets(self, force_conversion=False):
        '''
        Download all top-level assets from top-level folders specified in config['asset-sources']
        '''
        # In case somebody uses this feature to write to a production server :(
        clean_path = lambda p: os.path.normpath(p.replace('\0','').replace('..','').strip('/'))

        destination = clean_path(self.config['asset-path'])
        with script_subdirectory(destination):
            for source in self.config['asset-sources']:
                folder = self.root.folder(source)
                for file in folder.files:
                    convert = force_conversion
                    if file.save_as(file.title):
                        log('download: asset "{}"'.format(file.title))
                        convert = True
                    if convert and file.attributes['mimeType'] in ('image/png', 'image/jpeg'):
                        log('convert: asset "{}"'.format(file.title))
                        for prefix,width in self.config['asset-image-sizes'].items():
                            Popen([
                                'convert', 
                                '-quality', self.config['asset-image-quality'], 
                                '-resize', width + 'x', 
                                file.title, prefix + file.title
                            ])


if __name__ == '__main__':
    with only_one_process(DB_NAME):
        ContentLoader()

